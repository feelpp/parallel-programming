<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>TPU (Tensor Processing Unit) form Google :: Parallel Programming</title>
    <link rel="canonical" href="https://feelpp.github.io/parallel-programming/parallel-programming/PPChapter1_TPU.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../_/css/site.css">
<link rel="icon" href="../_/img/favicon.ico" type="image/x-icon">
<script>!function(l,p){if(l.protocol!==p&&l.host=="docs.antora.org"){l.protocol=p}else if(/\.gitlab\.io$/.test(l.host)){l.replace(p+"//docs.antora.org"+l.pathname.substr(l.pathname.indexOf("/",1))+l.search+l.hash)}}(location,"https:")</script>

<script src="../_/js/vendor/tabs-block-extension.js"></script>
<script src="../_/js/vendor/tabs-block-behavior.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },

  TeX: {
      Macros: {
      bold: ["{\\bf #1}",1],
      calTh: "{\\mathcal{T}_h}",
      card: ["{\\operatorname{card}(#1)}",1],
      card: ["{\\operatorname{card}(#1)}",1],
      Ck: ["{\\mathcal{C}^{#1}}",1],
      deformt: ["{\\mathbf{\\varepsilon(#1)}}",1],
      diam: "{\\operatorname{diam}}",
      dim: ["{\\operatorname{dim}(#1)}",1],
      disp: ["{\\mathbf{#1}}",1],
      domain: "{\\Omega}",
      ds: "",
      essinf: "{\\operatorname{ess}\\, \\operatorname{inf}}",
      F:"{\\mathcal{F}}",
      geo: "{\\mathrm{geo}}",
      Ich: ["{\\mathcal{I}^{#1}_{c,h}#2}",2],
      Id: "{\\mathcal{I}}",
      Ilag: ["{\\mathcal{I}^{\\mathrm{lag}}_{#1}}",1],
      jump: ["{[\\![ #1 ]\\!]}",1],
      n:"{\\mathbf{n}}",
      Ne: "{N_{\\mathrm{e}}}",
      Next: "{\\mathrm{n}}",
      nf: "{n_f}",
      ngeo: "{n_{\\mathrm{geo}}}",
      Nma: "{N_{\\mathrm{ma}}}",
      NN: "{\\mathbb N}",
      Nno: "{N_{\\mathrm{no}}}",
      Nso: "{N_{\\mathrm{so}}}",
      opdim: "{\\operatorname{dim}}",
      p: "{\\mathrm{p}}",
      P:"{\\mathcal{P}}",
      Pch: ["{P^{#1}_{c,h}}",1],
      Pcho: ["{P^{#1}_{c,h,0}}",1],
      Pk: ["{\\mathcal{P}^{#1}}",1],
      poly: ["{\\mathbb{#1}",1],
      poly: ["{\\mathbb{#1}}",1],
      prect: ["{\\left\\(#1\\right\\)}",1],
      q:"{\\mathbf{q}}",
      Qch: ["{Q^{#1}_{c,h}}",1],
      Qk: ["{\\mathcal{Q}^{#1}}",1],
      R: ["{\\mathbb{R}^{#1}}",1],
      RR: "{\\mathbb R}",
      set: ["{\\left\\{#1\\right\\}}",1],
      stresst: ["{\\mathbf{\\sigma(#1)}}",1],
      T:"{\\mathcal{T}}",
      tr: "{\\operatorname{tr}}",
      v:"{\\mathbf{v}}",
      vertiii: ["\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert #1 \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert",1]
  },
  extensions: ["mhchem.js"] 
  }
});
</script>
<!--<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script>-->
<!-- <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_CHTML'></script>
<!--<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.0/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>-->

<!--<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" integrity="sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js" integrity="sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq" crossorigin="anonymous"></script>-->
<script>var uiRootPath = '../_'</script>

  </head>
  <body class="article">
<header class="header">
    <nav class="navbar navbar-expand-sm bg-dark navbar-dark navbar-template-project" style="border-top: 4px solid #9E9E9E">
        <div class="navbar-brand">
            <div class="navbar-item feelpp-logo">
                <a href="https://feelpp.github.io/parallel-programming">Parallel Programming</a>
            </div>
            <button class="navbar-burger" data-target="topbar-nav">
                <span></span>
                <span></span>
                <span></span>
            </button>
        </div>

        <div id="topbar-nav" class="navbar-menu">
            <div class="navbar-end">
                <div class="navbar-item">
                    <a href="https://docs.feelpp.org/">Documentation Reference</a>
                </div>
                <div class="navbar-item has-dropdown is-hoverable download-item">
                    <div class="navbar-item"><a href="https://docs.feelpp.org/user/latest/install/index.html" class="download-btn">Get Feel++</a></div>
                </div>
                <div class="navbar-item">
                    <a class="navbar-brand"  href="https://www.cemosis.fr">
                        <img class="cemosis-logo"  src="../_/img/cemosis-logo.svg" alt="Cemosis logo"/>
                    </a>
                </div>
            </div>
        </div>
    </nav>
</header>
<div class="body">
<a href="#" class="menu-expand-toggle"></a>
<div class="nav-container" data-component="parallel-programming" data-version="">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="index.html">Main</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="index.html">Introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter1_CPU.html">CPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter1_GPU.html">GPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter1_GPGPU.html">GPGPU Architecture</a>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="PPChapter1_TPU.html">TPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter1_SIMD.html">SIMD Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter1_AMD_CUDA.html">AMD CUDA Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter2_MPI.html">MPI (Message Passing Interface)</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter2_MPI_Boost.html">MPI Boost</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter2_OpenMP.html">OpenMP (Open Multi-Processing)</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter2_OpenMP2.html">OpenMP more information</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter2_Hybrid.html">Hybrid MPI with OpenMP</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter3.html">StarPU</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter4.html">Specx</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Main</span>
    <span class="version"></span>
  </div>
  <ul class="components">
      <li class="component">
        <a class="title" href="../feelpp-antora-ui/index.html">Antora Feel++ UI</a>
      </li>
      <li class="component is-current">
        <a class="title" href="index.html">Main</a>
      </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
  <button class="nav-toggle"></button>
    <a href="index.html" class="home-link"></a>
  <nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="index.html">Main</a></li>
    <li><a href="PPChapter1_TPU.html">TPU Architecture</a></li>
  </ul>
</nav>

  
    <div class="edit-this-page"><a href="https://github.com/feelpp/parallel-programming/edit/lem/docs/modules/ROOT/pages/PPChapter1_TPU.adoc">Edit this Page</a></div>
  
  <div class="page-downloads">
  <span class="label">Download as</span>
  <ul class="download-options">
    <li>
      <a onclick="print(this)" href="#" data-toggle="tooltip" data-placement="left" title="Print to PDF"
         class="pdf-download">
        <img class="pdf-file-icon icon" src="../_/img/pdf.svg"/> .pdf
      </a>
    </li>
  </ul>
</div>
</div>

  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="3">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">TPU (Tensor Processing Unit) form Google</h1>
<div id="preamble">
<div class="sectionbody">
<div class="imageblock">
<div class="content">
<a class="image" href="#fragment03"><img src="_images/GPGPU.jpg" alt="GPGPU" width="322" height="220"></a>
</div>
</div>
<div class="paragraph text-justify">
<p>A Tensor Processing Unit (TPU) is a specialized hardware processor developed by Google to accelerate machine learning. Unlike traditional CPUs or GPUs, TPUs are specifically designed to handle tensor operations, which account for most of the computations in deep learning models. This makes them incredibly efficient at those tasks and provides an enormous speedup compared to CPUs and GPUs. In this article, we’ll explore what a TPU is, how it works, and why they are so beneficial for machine learning applications.</p>
</div>
</div>
</div>
<div class="sect1 text-justify">
<h2 id="_what_are_tensor_processing_units_tpu"><a class="anchor" href="#_what_are_tensor_processing_units_tpu"></a>1. What Are Tensor Processing Units (TPU)?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Tensor Processing Unit (TPU) is an application-specific integrated
circuit (ASIC) designed specifically for machine learning. In addition, TPUs offer improved energy efficiency, allowing businesses to reduce their electricity bills while still achieving the same results as processors with greater energy consumption. This makes them an attractive option for companies looking to use AI in their products or services. With the help of TPUs, businesses can develop and deploy faster, more efficient models that are better suited to their needs. TPUs offer a range of advantages over CPUs and GPUs. For instance, they provide up to 30x faster performsance than traditional processors and up to 15x better energy efficiency. This makes them ideal for companies looking to develop complex models in a fraction of the time. Finally, TPUs are more affordable than other specialized hardware solutions, making them an attractive option for businesses of all sizes.</p>
</div>
<div class="paragraph">
<p>Tensor Processing Units are Google&#8217;s ASIC for machine learning. TPUs are specifically used for deep learning to solve complex matrix and vector operations. TPUs are streamlined to solve matrix and vector operations at ultra-high speeds but must be paired with a CPU to give and execute instructions.</p>
</div>
<div class="paragraph">
<p><span class="image"><a class="image" href="#fragment22"><img src="_images/image22.png" alt="image22" width="544" height="419"></a></span></p>
</div>
</div>
</div>
<div class="sect1 text-justify">
<h2 id="_is_google_tpu_faster_than_gpu"><a class="anchor" href="#_is_google_tpu_faster_than_gpu"></a>2. Is Google TPU faster than GPU?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>GPUs have the ability to break complex problems into thousands or millions of separate tasks and work them out all at once, while TPUs were designed specifically for neural network loads and have the ability to work quicker than GPUs while also using fewer resources.</p>
</div>
</div>
</div>
<div class="sect1 text-justify">
<h2 id="_key_differences_between_tpu_and_gpu"><a class="anchor" href="#_key_differences_between_tpu_and_gpu"></a>3. Key differences between TPU and GPU</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Architecture: While GPUs use a flexible, general-purpose architecture, TPUs are purpose-built for machine learning tasks. GPUs consist of thousands of small cores designed to handle multiple tasks simultaneously, whereas TPUs have a more streamlined architecture focused on accelerating tensor operations.</p>
</li>
<li>
<p>Performance: When it comes to raw performance, TPUs have an edge over GPUs in certain scenarios. TPUs are designed to perform lower-precision calculations with higher throughput, which is often sufficient for training and inference tasks in neural networks. However, GPUs offer greater flexibility in terms of precision and can handle higher-precision computations when necessary.</p>
</li>
<li>
<p>Memory and Bandwidth: TPUs typically have a higher memory bandwidth than GPUs, which allows them to handle large tensor operations more efficiently. This results in faster training and inference times for neural networks. However, the amount of memory available on TPUs is generally lower than on GPUs, which can be a limiting factor for some applications.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_pros_and_cons"><a class="anchor" href="#_pros_and_cons"></a>4. Pros and Cons</h2>
<div class="sectionbody">
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 24%;">
<col style="width: 76%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">GPU Pros</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Flexibility</strong>: GPUs can handle a wide range of tasks, including graphics rendering, simulations, and scientific computing, in addition to machine learning workloads.</p>
<p class="tableblock"><strong>Maturity</strong>: GPUs have been widely adopted for deep learning, and there is a vast ecosystem of software and tools built around them, such as CUDA, cuDNN, and popular deep learning frameworks like TensorFlow and PyTorch.</p>
<p class="tableblock"><strong>Precision</strong>: GPUs offer a range of precision options, from low-precision FP16 to high-precision FP64, making them suitable for various workloads with different accuracy requirements.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">GPU Cons</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Power Consumption</strong>: GPUs typically consume more power than TPUs, which can be a concern for large-scale deployments and energy efficiency.</p>
<p class="tableblock"><strong>Cost</strong>: High-performance GPUs can be expensive, especially for small businesses or individual researchers.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 24%;">
<col style="width: 76%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">TPU Pros</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Performance</strong>: TPUs are designed specifically for tensor operations, resulting in faster training and inference times for neural networks compared to GPUs.</p>
<p class="tableblock"><strong>Energy Efficiency</strong>: TPUs are more power-efficient than GPUs, making them a better choice for large-scale machine learning deployments.</p>
<p class="tableblock"><strong>Ease of use</strong>: TPUs are integrated with popular machine learning frameworks like TensorFlow, making it easy for developers to leverage their capabilities.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">TPU Cons</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Limited Ecosystem</strong>: The TPU ecosystem is less mature than that of GPUs, with fewer software and tools available.</p>
<p class="tableblock"><strong>Availability</strong>: TPUs are primarily available through Google Cloud Platform, which may not be suitable for all users and organizations.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect1 text-justify">
<h2 id="_is_tpu_faster_than_gpu_for_pytorch"><a class="anchor" href="#_is_tpu_faster_than_gpu_for_pytorch"></a>5. Is TPU faster than GPU for PyTorch?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>TPUs typically have a higher memory bandwidth than GPUs, which allows them to handle large tensor operations more efficiently. This results in faster training and inference times for neural networks.</p>
</div>
</div>
</div>
<div class="sect1 text-justify">
<h2 id="_applications_for_tpus"><a class="anchor" href="#_applications_for_tpus"></a>6. Applications for TPUs</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Tensor Processing Units (TPUs) are specialized ASIC chips designed to accelerate the performance of machine learning algorithms. They can be used in a variety of applications, ranging from cloud computing and edge computing to machine learning or in various deep learning applications such as fraud detection, computer vision, natural language processing, self-driving cars, vocal AI, agriculture, virtual assistants, stock trading, e-commerce, and various social predictions By leveraging the power of TPUs, organizations can reduce costs and optimize their operations.</p>
</div>
<div class="paragraph">
<p><strong>Cloud Computing:</strong> TPUs are used in cloud computing to provide better performance for workloads that require a lot of data processing. This allows businesses to process large amounts of data quickly and accurately at a lower cost than ever before. With the help of TPUs, businesses can make more informed decisions faster and improve their operational efficiency.</p>
</div>
<div class="paragraph">
<p><strong>Edge Computing:</strong> TPUs are also used in edge computing applications, which involve processing data at or near the source. This helps to reduce latency and improve performance for tasks such as streaming audio or video, autonomous driving, robotic navigation, and predictive analytics. Edge computing also facilitates faster and more reliable communication between devices in an IoT network.</p>
</div>
<div class="paragraph">
<p><strong>Machine Learning:</strong> TPUs are used to accelerate machine learning models and algorithms. They can be used to develop novel architectures that are optimized for tasks such as natural language processing, image recognition, and speech recognition. By leveraging the power of TPUs organizations can develop more complex models and algorithms faster. This will enable them to achieve better results with their machine-learning applications.</p>
</div>
<div class="paragraph">
<p><strong><em>When to Use TPUs</em></strong>: Since TPUs are high specialized hardware for deep learning, it loses a lot of other functions you would typically expect from a general-purpose processor like a CPU. With this in mind, there are specific scenarios where using TPUs will yield the best result when training AI. The best time to use a TPU is for operations where models rely heavily on matrix computations, like recommendation systems for search engines. TPUs also yield great results for models where the AI analyzes massive amounts of data points that will take multiple weeks or months to complete. AI engineers use TPUs for instances without custom TensorFlow models and have to start from scratch.</p>
</div>
<div class="paragraph">
<p><strong><em>When Not to Use TPUs</em></strong>: As stated earlier, the optimization of TPUs causes these types of processors to only work on specific workload operations. Therefore, there are instances where opting to use a traditional CPU and GPU will yield faster results. These instances include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Rapid prototyping with maximum flexibility</p>
</li>
<li>
<p>Models limited by the available data points</p>
</li>
<li>
<p>Models that are simple and can be trained quickly</p>
</li>
<li>
<p>Models too onerous to change</p>
</li>
<li>
<p>Models reliant on custom TensorFlow operations written in C++</p>
</li>
</ul>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 14%;">
<col style="width: 86%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>TPU Versions and Specifications</strong></p></td>
<td class="tableblock halign-left valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>TPUv1</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The first publicly announced TPU. Designed as an 8-bit matrix multiplication engine and is limited to solving only integers.
[.text-justify]</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>TPUv2</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Since engineers noted that TPUv1 was limited in bandwidth. This version now has double the memory bandwidth with 16GB of RAM. This version can now solve floating points making it useful for training and inferencing.
[.text-justify]</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>TPUv3</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Released in 2018, TPUv3 has twice the processors and is deployed with four times as many chips as TPUv2. The upgrades allow this version to have eight times the performance over previous versions.
[.text-justify]</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>TPUv4</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">version of TPU produced on May 18, 2021. Google&#8217;s CEO announced that this version would have more than twice the performance of TPU v3.
[.text-justify]</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>TPUv5</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">This is the latest version of TPU on Dec 6, 2023. Compared to TPU v4, TPU v5p features more than 2X greater FLOPS and 3X more high-bandwidth memory (HBM),” Google said. The TPU v5p pods have 95GB of high-bandwidth memory, while the TPU v4 pods have 32GB of HBM. The HBM bandwidth of a TPU v5p pod is 2,765GBps, while the TPU v4 bandwidth was 1228GBps.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Edge TPU</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">This TPU version is meant for smaller operations optimized to use less power than other versions of TPU in overall operation. Although only using two watts of power, Edge TPU can solve up to four terra-operations per second. Edge TPU is only found on small handheld
devices like Google&#8217;s Pixel 4 smartphone.</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 26%;">
<col style="width: 74%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"><strong>Benefits of the TPU Architecture</strong></th>
<th class="tableblock halign-left valign-top"></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>High Performance</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is designed to maximize performance, ensuring that the processor can execute operations at extremely high speeds.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Low Power Consumption</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Compared to CPUs and GPUs, the TPU architecture requires significantly less power consumption, making it ideal for applications in which energy efficiency is a priority.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Cost Savings</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is designed to be affordable, making it an attractive solution for businesses that are looking to reduce their hardware costs.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Scalability</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is highly scalable and can
accommodate a wide range of workloads, from small applications to large-scale projects.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Flexibility</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is flexible and can be adapted to meet the needs of different applications, making it suitable for a range of use cases.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Efficient Training</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture enables efficient training of deep learning models, allowing businesses to quickly iterate and improve their AI solutions.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Security</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is highly secure, making it an ideal solution for mission-critical applications that require high levels of security.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Enhanced Reliability</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture has enhanced reliability, providing businesses with the assurance that their hardware will perform as expected in any environment.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Easy to Deploy</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is designed for easy deployment, allowing businesses to quickly set up and deploy their hardware solutions.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Open Source Support</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is backed by an open-source community that provides support and assistance when needed, making it easier for businesses to get the most out of their hardware investments.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Improved Efficiency</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is designed to optimize efficiency, allowing businesses to get the most out of their hardware resources and reducing the cost of running AI applications.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>End-to-End Solutions</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture provides a complete end-to-end solution for all types of AI projects, allowing businesses to focus on their development and operations instead of worrying about hardware compatibility.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Cross-Platform Support</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is designed to work across multiple platforms, making it easier for businesses to deploy their AI solutions in any environment.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Future Ready</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is designed with the future in mind, providing businesses with a solution that will remain up-to-date and ready to take on next-generation AI applications.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Industry Standard</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The TPU architecture is becoming an industry standard for AI applications, giving businesses the confidence that their hardware investments are future-proofed.</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect1 text-justify">
<h2 id="_google_tpu_v5e_ai_chip"><a class="anchor" href="#_google_tpu_v5e_ai_chip"></a>7. Google TPU v5e AI Chip</h2>
<div class="sectionbody">
<div class="paragraph">
<p>TPU v5e is also Google&#8217;s first AI chip integrated into a suite of software and tools for large-scale orchestration of AI workloads in virtual environments. The AI chip is now available in preview to Google Cloud customers.</p>
</div>
<div class="paragraph">
<p>The new AI chip succeeds the previous generation TPUv4, which was used to train the new large language models PaLM and PaLM 2 used in Google search, mapping and online productivity applications.
The Cloud TPU v5e is also the first Google AI chip available outside the United States. TPUv4 was only available in North America. The TPU v5e computers will be installed in the Netherlands for EMEA markets and in Singapore for Asia-Pacific markets.</p>
</div>
<div class="paragraph">
<p>A researcher, Andrew B. Kahng of the University of California, San Diego, then reverse-engineered Google&#8217;s chip design techniques and found that human chip designers and automated tools were sometimes faster than the technique based solely on Google&#8217;s AI.</p>
</div>
<div class="paragraph">
<p>The performance numbers indicate that the TPU v5e is suited for inference rather than training. The chip delivers a peak performance of 393 teraflops of INT8 performance per chip, which is better than 275 petaflops on TPU v4.</p>
</div>
<div class="paragraph">
<p>The new Google TPU v5e is more efficient and more scalable than v4, according to Google.  Multislice “enables users to easily scale AI models beyond the limits of physical TPU modules – up to tens of thousands of Cloud TPU v5e or TPU v4 chips,”. Google has also tweaked the virtual machines for TPU v5e so that the chips can process multiple virtual machines simultaneously. Google announced the availability of Kubernetes service for Cloud TPU v5e and v4, which will help orchestrate AI workloads on TPUs. Google said the largest configuration could deploy 64 virtual machines across 256 TPU v5e clusters. “This feature allows customers to choose the right configurations to serve a wide range of LLM and gen AI model sizes,” Google executives wrote.</p>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer" style="border-top: 2px solid #e9e9e9; background-color: #fafafa; padding-bottom: 2em; padding-top: 2em;">
    <div class="container" style="display: flex; flex-direction: column; align-items: center; gap: 0.5em;">
        <div>
            <a href="https://www.cemosis.fr">
                <img src="../_/img/cemosis-logo.svg" alt="Cemosis logo" height="50">
            </a>
        </div>
        <span style="font-size: 0.8rem; color: #9e9e9e">© 2024 <a href="https://www.cemosis.fr" style="text-decoration: underline;">Cemosis</a>, Université de Strasbourg</span>
    </div>
</footer>
<script id="site-script" src="../_/js/site.js" data-ui-root-path="../_"></script>


<script async src="../_/js/vendor/fontawesome-icon-defs.js"></script>
<script async src="../_/js/vendor/fontawesome.js"></script>
<script async src="../_/js/vendor/highlight.js"></script>


<script type="text/javascript">
function toggleFullScreen() {
   var doc = window.document;
   var docEl = doc.documentElement;

   var requestFullScreen = docEl.requestFullscreen || docEl.mozRequestFullScreen || docEl.webkitRequestFullScreen || docEl.msRequestFullscreen;
   var cancelFullScreen = doc.exitFullscreen || doc.mozCancelFullScreen || doc.webkitExitFullscreen || doc.msExitFullscreen;

   if(!doc.fullscreenElement && !doc.mozFullScreenElement && !doc.webkitFullscreenElement && !doc.msFullscreenElement) {
       requestFullScreen.call(docEl);
   }
   else {
       cancelFullScreen.call(doc);
   }
}
</script>
  </body>
</html>
