=== 2. Programming interface for parallel computing

*MPI, OpenMP two complementary parallelization models.*
[.text-justify]
– MPI is a multi-process model whose mode of communication between the
processes is *explicit* (communication management is the responsibility
of the user). MPI is generally used on multiprocessor machines with
distributed memory. MPI is a library for passing messages between
processes without sharing.
[.text-justify]
– OpenMP is a multitasking model whose mode of communication between
tasks is *implicit* (the management of communications is the
responsibility of the compiler). OpenMP is used on shared-memory
multiprocessor machines. It focuses on shared memory paradigms. It is a
language extension for expressing data-parallel operations (usually
parallelized arrays over loops).
[.text-justify]
Note: on a cluster of independent shared-memory multiprocessor machines
(nodes), the implementation of a two-level parallelization (MPI, OpenMP)
in the same program can be a major advantage for the parallel
performance of the code.

image::image7.png[xref=#fragment7,width=581,height=336]


[width="100%",cols="50%,50%",]
|===
|*MPI vs. OpenMP* |
|*MPI pos* |*OpenMP pos*
a|
Portable to a distributed and shared memory machine.

Scale beyond a node

No data placement issues

a|
Easy to implement parallelism

Implicit communications

Low latency, high bandwidth

Dynamic Load Balancing

|*MPI negative* |*OpenMP negative*
a|
Explicit communication

High latency, low bandwidth

Difficult load balancing

a|
Only on nodes or shared memory machines

Scale on Node

Data placement problem

|===

*2.1 MPI (Message Passing Interface)*

*Point-to-point communications*

*General notions*
[.text-justify]
The transmitter and the receiver are identified by their rank in the
communicator. The entity passed between two processes is called a
message . +
A message is characterized by its envelope . This consists of:
[.text-justify]
• the rank of the sending process; +
• the rank of the receiving process; +
• the label ( _tag_ ) of the message; +
• the communicator who defines the process group and the communication
context.
[.text-justify]
The data exchanged is typed (integers, reals, etc. or personal derived
types).
[.text-justify]
In each case, there are several transfer modes , using different
protocols.
    
    int MPI_Send( *const void* *message, *int* length, MPI_Datatype
    type_message, *int* rank_dest, *int* label, MPI_Comm comm)
    
    int MPI_Recv ( *void* *message, *int* length, MPI_Datatype
    type_message, *int* rank_source, *int* label, MPI_Comm comm, MPI_Status
    *status)

Note this operation is blocking.

*Simultaneous send and receive operation*

    int MPI_Sendrecv ( *const void* *message_sent, *int*
    length_message_sent, +
    MPI_Datatype type_message_sent, *int* rank_dest, *int*
    label_message_sent, *void* *message_received , *int*
    length_message_received, +
    MPI_Datatype type_message_received, *int* rank_source, *int*
    label_message_received, MPI_Comm comm, MPI_Status *status)

*Simultaneous send and receive operation*
    
    int MPI_Sendrecv_replace ( void * message, int length, MPI_Datatype
    type_message, int rank_dest, int label_message_sent, int* rank_source,
    int label_message_recu, MPI_Comm comm, MPI_Status *status)

*Collective communications*

*General notions*
[.text-justify]
Collective communications allow a series of point-to-point communications to be made in a single operation. A collective communication always concerns all the processes of the indicated communicator. For each of the processes, the call ends when the latter's participation in the collective operation is completed, in the sense of point-to-point
communications (thus when the memory zone concerned can be modified). The management of labels in these communications is transparent and at the expense of the system. They are therefore never explicitly defined
during the call to these subroutines. One of the advantages of this is that collective communications never interfere with point-to-point communications.

*Types of collective communications*
[.text-justify]
There are three types of subroutines: +
*1.* the one that ensures global synchronizations: MPI_Barrier() .

*2.* those that only transfer data:

• global data broadcasting: MPI_Bcast(); +
• selective diffusion of data: MPI_Scatter(); +
• distributed data collection: MPI_Gather(); +
• collection by all distributed data processes: MPI_Allgather(); •
selective collection and dissemination, by all processes, of distributed
data: MPI_Alltoall() .

*3.* those who, in addition to managing communications, perform
operations on the transferred data:

* {blank}
+

reduction operations (sum, product, maximum, minimum, etc.), whether of
a predefined type or of a personal type: MPI_Reduce();

* {blank}
+

reduction operations with distribution of the result (equivalent to an
MPI_Reduce() followed by an MPI_Bcast()): MPI_Allreduce().


*Global synchronization*

    int MPI_Barrier ( MPI_Comm comm)

*General distribution*

    int MPI_Bcast( void *message, int length, MPI_Datatype,
    type_message, *int* rank_source, MPI_Comm comm)

*Selective dissemination*

    int MPI_Scatter ( const void *message_to_be restarted, int
    length_message_sent, MPI_Datatype type_message_sent, void
    *message_received, int length_message_recu, MPI_Datatype type_message_recu, int
    rank_source, MPI_Comm comm)

*Collection*

    int MPI_Gather ( const void *message_sent, int
    length_message_sent, MPI_Datatype type_message_sent, void
    *message_received, int length_message_received, MPI_Datatype
    type_message_received, *int* rank_dest, MPI_Comm comm)

*General collection*

    int MPI_Allgather ( const void *message_sent, int
    length_message_sent, MPI_Datatype type_message_sent, void
    *message_received, int length_message_received, MPI_Datatype
    type_message_received, MPI_Comm comm)

*"Variable" collection*

    int MPI_Gatherv ( const void *message_sent, int
    length_message_sent, MPI_Datatype type_message_sent, void
    *message_received, const int *nb_elts_recus, const int *deplts,
    MPI_Datatype type_message_recu, *int* rang_dest, MPI_Comm comm)

*Selective collections and distributions*

    int MPI_Alltoall ( const void *message_sent, int
    length_message_sent, MPI_Datatype type_message_sent, void
    *message_received, int length_message_received, MPI_Datatype
    type_message_received, MPI_Comm comm)

*Distributed reductions*

    int MPI_Reduce ( const void *message_sent, void *message_received,
    int length, MPI_Datatype type_message, MPI_Op operation, int rank_dest,*
    MPI_Comm comm)

*Distributed reductions with distribution of the result*

    int MPI_Allreduce ( const void *message_sent, void *message_received, *int* length, MPI_Datatype, type_message, MPI_Op operation, MPI_Comm comm)



*Communication models*

*Point-to-point sending modes*

    _Blocking and Non-blocking mode_
    
    Standard sending MPI_Send() MPI_Isend()
    
    Synchronous send MPI_Ssend() MPI_Issend()
    
    _Buffered_ send MPI_Bsend() MPI_Ibsend()
    
    Receive MPI_Recv() MPI_Irecv()


*_Blocking calls_*

A call is blocking if the memory space used for communication can be
reused immediately after the call exits.

The data sent can be modified after the blocking call.

The received data can be read after the blocking call.


*Synchronous sends*
[.text-justify]
A synchronous send involves synchronization between the processes
involved. A shipment can only begin when its receipt is posted. There
can only be communication if both processes are willing to communicate.

*int* MPI_Ssend( *const void* * values, *int* size, MPI_Datatype
message_type, *int* dest, *int* label, MPI_Comm comm)


*Benefits*

Consume few resources (no _buffer_ ) +
Fast if the receiver is ready (no copying into a _buffer_ ) Recognition
of reception thanks to synchronization

*Disadvantages*

Waiting time if the receiver is not there/not ready Risks of deadlock


**_Buffered +
_**sends A buffered send involves the copying of data into an
intermediate memory space. There is then no coupling between the two
communication processes. The output of this type of sending therefore
does not mean that the reception has taken place.

Buffers must be managed manually (with calls to MPI_Buffer_attach( _)_
and MPI_Buffer_detach()). They must be allocated taking into account the
memory overhead of the messages (by adding the MPI_BSEND_OVERHEAD
constant for each message instance).

    int MPI_Buffer_attach ( void *buf, int size_buf) 
    int MPI_Buffer_detach ( void *buf, int size_buf) 
    int MPI_Bsend( const void *values, int size, MPI_Datatype type_message, int dest, int label, MPI_Comm comm)


*Advantages of buffered mode*

No need to wait for the receiver (recopy in a _buffer_ ) No risk of
blocking ( _deadlocks_ )

*Disadvantages of buffered mode*

Consume more resources (memory occupation by _buffers_ with risk of
saturation)

Send buffers must be managed manually (often difficult to choose an
appropriate size _)_
[.text-justify]
A bit slower than synchronous sends if the receiver is ready. No knowledge of the reception (send-receive decoupling). Risk of wasting memory space if the _buffers_ are too oversized. The application crashes if the _buffers_ are too small. There are also often hidden _buffers_ managed by the MPI implementation on the sender and/or receiver side (and consuming memory resources)

*Standard shipments*

MPI_Send() subroutine . In most implementations, this mode switches from
buffered _(_ eager _)_ to synchronous mode as message sizes grow.

    int MPI_Send( const void *values, int size, MPI_Datatype type_message, int dest, int label, MPI_Comm comm)


*Benefits of standard mode*

=> Often the most efficient (choice of the most suitable mode by the
manufacturer)

*Disadvantages of standard mode*

=> Little control over the mode actually used (often accessible via
environment variables)

Risk of _deadlock_ depending on the real mode +
Behavior may vary depending on the architecture and the size of the
problem

*Non-blocking calls*
[.text-justify]
non-blocking call returns control very quickly, but does not allow the
immediate reuse of the memory space used in the call. It is necessary to
ensure that the communication is indeed terminated (with MPI_Wait() for
example) before using it again.

    int MPI_Isend( const void *values, int size, MPI_Datatype
    message_type, int dest, int label, MPI_Comm comm, MPI_Request *req)
    
    int MPI_Issend ( const void* values, int size, MPI_Datatype
    message_type, int dest, int label, MPI_Comm comm, MPI_Request *req)
    
    int MPI_Ibsend( const void* values, int size, MPI_Datatype
    message_type, int dest, int label, MPI_Comm comm, MPI_Request *req)
    
    int MPI_Irecv( void *values, int size, MPI_Datatype type_message,
    int* source, int label, MPI_Comm comm, MPI_Request *req)


*Benefits of non-blocking calls*
[.text-justify]
Ability to hide all or part of the communication costs (if the
architecture allows it)

No risk of _deadlock_

*Disadvantages of non-blocking calls*
[.text-justify]
Higher additional costs (several calls for a single send or receive,
request management)

Higher complexity and more complicated maintenance

Risk of loss of performance on the calculation cores (for example
differentiated management between the zone close to the border of a
domain and the interior zone resulting in less good use of memory
caches)

Limited to point-to-point communications (has been extended to
collectives in MPI 3.0)

*interfaces*

MPI_Wait() waits for the end of a communication. MPI_Test() is the
non-blocking version.

    int MPI_Wait ( MPI_Request *req, MPI_Status *status) 
    int MPI_Test( MPI_Request *req, int *flag, MPI_Status *status)

MPI_Waitall() waits for all communications to end. MPI_Testall() is the
non-blocking version.

    int MPI_Waitall ( int size, MPI_Request reqs[], MPI_Status statuses[]) 
    int* MPI_Testall ( int size, MPI_Request reqs[], int *flag, MPI_Status statuses[])

MPI_Waitany waits for the end of one communication among several.

    int MPI_Waitany ( int size, MPI_Request reqs[], int *index,MPI_Status *status)

MPI_Testany is the non-blocking version. 

    int* MPI_Testany( int size, MPI_Request reqs[], int *index, int *flag, MPI_Status *status)

MPI_Waitsome is waiting for the end of one or more communications.

    int MPI_Waitsome( int size, MPI_Request reqs[], int *endcount,int *indexes, MPI_Status *status)

MPI_Testsome is the non-blocking version.

    int MPI_Testsome( int size, MPI_Request reqs[], int *endcount,int *indexes, MPI_Status *status)

*Memory-to-memory communications (RMA)*
[.text-justify]
Memory-to-memory communications (or RMA for _Remote Memory Access_ or
_one-sided communications_ ) consist of accessing the memory of a remote
process in write or read mode without the latter having to manage this
access explicitly. The target process therefore does not intervene
during the transfer.

*RMA - General Approach*
[.text-justify]
Creation of a memory window with MPI_Win_create() to authorize RMA
transfers in this area.

Remote read or write access by calling MPI_Put(), MPI_Get(),
MPI_Accumulate(), , MPI_Get_accumulate() and MPI_Compare_and_swap()

Freeing the memory window with M PI_Win_free() .

*RMA - Synchronization Methods*
[.text-justify]
To ensure correct operation, it is mandatory to carry out certain
synchronizations. 3 methods are available:

Active target communication with global synchronization (
MPI_Win_fence() );

Communication with active target with pair synchronization
(MPI_Win_start() and MPI_Win_complete() for the origin process;
MPI_Win-post() and MPI_Win_wait() for the target process);

Passive target communication without target intervention (MPI_Win_lock()
and MPI_Win_unlock()).

*Benefits of RMAs*
[.text-justify]
Allows you to implement certain algorithms more efficiently.

More efficient than point-to-point communications on some machines (use
of specialized hardware such as DMA engine, coprocessor, specialized
memory, etc.).

Ability for the implementation to group multiple operations.

*Disadvantages of RMAs*
[.text-justify]
Synchronization management is tricky.

Complexity and high risk of error.

For passive target synchronizations, obligation to allocate memory with
MPI_Alloc_mem() which does not respect the Fortran standard (use of Cray
pointers not supported by some compilers).

Less efficient than point-to-point communications on some machines.

*Derived data types*
[.text-justify]
In the communications, the data exchanged are typed: MPI_INTEGER,
MPI_REAL, MPI_COMPLEX, etc .

More complex data structures can be created using subroutines such as
MPI_Type_contiguous(), MPI_Type_vector(), MPI_Type_Indexed() , or
MPI_Type_create_struct()

The derived types notably allow the exchange of non-contiguous or
non-homogeneous data in memory and to limit the number of calls to the
communications subroutines.

*MPI keywords*

[width="100%",cols="50%,50%",]
|===
a|
*1 environment*

• MPI Init: Initialization of the MPI environment

• MPI Comm rank: Rank of the process

• MPI Comm size: Number of processes

• MPI Finalize: Deactivation of the MPI environment 

• MPI Abort:Stopping of an MPI program

• MPI Wtime: Time taking

*2 Point-to-point communications*

• MPI Send: Send message

• MPI Isend: Non-blocking message sending

• MPI Recv: Message received

• MPI Irecv: Non-blocking message reception

• MPI Sendrecv and MPI Sendrecv replace: Sending and receiving messages

• MPI Wait: Waiting for the end of a non-blocking communication

• MPI Wait all: Wait for the end of all non-blocking communications

*3 Collective communications*

• MPI Bcast: General broadcast

• MPI Scatter: Selective spread

• MPI Gather and MPI Allgather: Collecting

• MPI Alltoall: Collection and distribution

• MPI Reduce and MPI Allreduce: Reduction 

• MPI Barrier: Global synchronization

*4 Derived Types*

• MPI Contiguous type: Contiguous types

• MPI Type vector and MPI Type create hvector: Types with a con-standing

• MPI Type indexed: Variable pitch types

• MPI Type create subarray: Sub-array types

• MPI Type create struct: H and erogenous types

• MPI Type commit: Type commit

• MPI Type get extent: Recover the extent

• MPI Type create resized: Change of scope

• MPI Type size: Size of a type

• MPI Type free: Release of a type

a|
*5 Communicator*

• MPI Comm split: Partitioning of a communicator

• MPI Dims create: Distribution of processes

• MPI Cart create: Creation of a Cart ́esian topology

• MPI Cart rank: Rank of a process in the Cart ́esian topology

• MPI Cart coordinates: Coordinates of a process in the Cart ́esian
topology

• MPI Cart shift: Rank of the neighbors in the Cart ́esian topology

• MPI Comm free: Release of a communicator

*6 MPI-IO*

• MPI File open: Opening a file

• MPI File set view: Changing the view • MPI File close: Closing a file

*6.1 Explicit addresses*

• MPI File read at: Reading

• MPI File read at all: Collective reading

• MPI File write at: Writing

*6.2 Individual pointers*

• MPI File read: Reading

• MPI File read all: collective reading

• MPI File write: Writing

• MPI File write all: collective writing

• MPI File seek: Pointer positioning

*6.3 Shared pointers*

• MPI File read shared: Read

• MPI File read ordered: Collective reading

• MPI File seek shared: Pointer positioning

*7.0 Symbolic constants*

• MPI COMM WORLD, MPI SUCCESS

• MPI STATUS IGNORE, MPI PROC NULL

• MPI INTEGER, MPI REAL, MPI DOUBLE PRECISION

• MPI ORDER FORTRAN, MPI ORDER C

• MPI MODE CREATE,MPI MODE RONLY,MPI MODE WRONLY

|===

*2.2 OpenMP (Open Multi-Processing)*

OpenMP ( Open Multi-Processing ) is a programming interface for parallel
computing on shared memory architecture.

It allows you to manage:

* {blank}
+

the creation of light processes,

* {blank}
+

the sharing of work between these lightweight processes,

* {blank}
+

synchronizations (explicit or implicit) between all light processes,

* {blank}
+

the status of the variables (private or shared).

*General concepts*

An OpenMP program is executed by a single process.
[.text-justify]
* This process activates lightweight processes (threads) at the entrance
to a parallel region. +
* Each thread performs a task consisting of a set of instructions. +
* During the execution of a task, a variable can be read and/or modified
in memory.

[.text-justify]
– It can be defined in the stack (local memory space) of a lightweight
process; we then speak of a private variable

– It can be defined in a shared memory space

[.text-justify]
* An OpenMP program is an alternation of sequential regions and parallel
regions. +
* A sequential region is always executed by the master task, the one
whose rank is 0. +
* A parallel region can be executed by several tasks at the same time. +
* The tasks can share the work contained in the parallel region.

* Work sharing essentially consists of:

[.text-justify]
– execute a loop by distributing the iterations between the tasks; +
– execute several sections of code but only one per task; +
– execute several occurrences of the same procedure by different tasks
(orphaning)

[.text-justify]
* It is sometimes necessary to introduce a synchronization between the
concurrent tasks to avoid, for example, that these modify in any order
the value of the same shared variable (case of reduction operations).
[.text-justify]
* Generally, tasks are assigned to processors by the operating system.
Different cases can occur:

[.text-justify]
– at best, at each instant, there is one task per processor with as many
tasks as there are dedicated processors for the duration of the work; +
– at worst, all tasks are processed sequentially by one and only one
processor; +
– in reality, for reasons essentially of operation on a machine whose
processors are not dedicated, the situation is generally intermediate.

[.text-justify]
* To overcome these problems, it is possible to build the OpenMP runtime
on a library of mixed threads and thus control the scheduling of tasks.

*Construction of a parallel region*
[.text-justify]
* In a parallel region, by default, the status of variables is shared.
* Within a single parallel region, all concurrent tasks execute the same
code.
* There is an implicit synchronization barrier at the end of the
parallel region.
* “Branching” (eg GOTO, CYCLE, etc.) into or out of a parallel region or
any other OpenMP construct is prohibited.
* It is possible, thanks to the DEFAULT clause, to change the default
status of variables in a parallel region.
* If a variable has a private status (PRIVATE), it is in the stack of
each task. Its value is then undefined at the entry of a parallel region
(in the example opposite, the variable a equals 0 at the entry of the
parallel region)
* However, thanks to the FIRSTPRIVATE clause, it is possible to force
the initialization of this private variable to the last value it had
before entering the parallel region.

*Extent of a parallel region*
[.text-justify]
* The scope of an OpenMP construct represents the scope of its influence
in the program. +
The influence (or scope) of a parallel region extends both to the code
contained lexically in this region (static scope), and to the code of
the called subroutines. The union of the two represents “dynamic
extent”.
* In a subroutine called in a parallel region, the local and automatic
variables are implicitly private to each of the tasks (they are defined
in the stack of each task).
* In a procedure, all the variables passed by argument (dummy
parameters) by reference, inherit the status defined in the lexical
scope (static) of the region.

*Case of static variables*
[.text-justify]
* A variable is static if its location in memory is defined at
declaration by the compiler
* Using the THREADPRIVATE directive allows you to privatize a static
instance and make it persistent from one parallel region to another. (
omp_get_thread_num(); )
* If, in addition, the COPYIN clause is specified then the value of
static instances is passed to all tasks.

*Case of dynamic allocation*
[.text-justify]
* The dynamic memory allocation/deallocation operation can be performed
inside a parallel region.
* If the operation relates to a private variable, it will be local to
each task.
* If the operation concerns a shared variable, then it is more prudent
that only one task (e.g. the master task) takes care of this operation

*Complements*
[.text-justify]
The construction of a parallel region admits two other clauses:

– REDUCTION: for reduction operations with implicit synchronization
between tasks; +
– NUM_THREADS: it allows to specify the desired number of tasks at the
entrance of a parallel region in the same way as the OMP_SET_NUM_THREADS
subroutine would do.
[.text-justify]
From one parallel region to another, the number of concurrent tasks can
be varied if desired. To do this, simply use the OMP_SET_DYNAMIC
subroutine or set the OMP_DYNAMIC environment variable to true. It is
possible to nest (nesting) parallel regions, but this only has an effect
if this mode has been activated by calling the OMP_SET_NESTED subroutine
or by setting the OMP_NESTED environment variable.

    *Examples*
    
    #include <omp.h>
    
    int main() 
    { 
    int row;
    
    #pragma omp parallel private(rank) num_threads(3) 
    { 
    rank=omp_get_thread_num(); 
    printf("My rank in region 1: %d \n",rank);
    
    #pragma omp parallel private(rank) num_threads(2) 
    { 
    rank=omp_get_thread_num(); 
    printf(" My rank in region 2: %d \n",rank); 
    }
    
    }
    return 0; 
    }
    
    My rank in region 1: 0 
    My rank in region 2: 1 
    My rank in region 2: 0 
    My rank in region 1: 2 
    My rank in region 2: 1 
    My rank in region 2: 0 
    My rank in region 1: 1 
    My rank in region 2: 0 
    My rank in region 2: 1

*Work sharing*
[.text-justify]
* In principle, building a parallel region and using a few OpenMP
functions alone is enough to parallelize a piece of code.
* But, in this case, it is up to the programmer to distribute the work
as well as the data and to ensure the synchronization of the tasks.
* Fortunately, OpenMP offers three directives (DO, SECTIONS and
WORKSHARE) which easily allow fairly fine control over the distribution
of work and data as well as synchronization within a parallel region.
* In addition, there are other OpenMP constructs that allow the
exclusion of all but one task to execute a piece of code located in a
parallel region.

*Parallel loop*
[.text-justify]
* It is a parallelism by distribution of the iterations of a loop.
[.text-justify]
* The parallelized loop is the one immediately following the DO
directive.
[.text-justify]
* "Infinite" and do while loops are not parallelizable with OpenMP.
[.text-justify]
* The mode of distribution of iterations can be specified in the SCHEDULE clause.
[.text-justify]
* Choosing the distribution mode provides more control over balancing the workload between tasks.
[.text-justify]
* Loop indices are private integer variables.
* By default, a global synchronization is performed at the end of the END DO construction unless the + NOWAIT clause has been specified.

*SCHEDULE clause*
[.text-justify]
* STATIC dispatching consists of dividing the iterations into packets of
a given size (except perhaps for the last one). A set of packets is then
assigned cyclically to each of the tasks, following the order of the
tasks up to the total number of packets. We could have deferred the
choice of the mode of distribution of the iterations using the
OMP_SCHEDULE environment variable. The choice of the distribution mode
of the iterations of a loop can be a major asset for balancing the
workload on a machine whose processors are not dedicated. Caution, for
vector or scalar performance reasons, avoid parallelizing loops
referring to the first dimension of a multi-dimensional array.
* DYNAMIC: iterations are divided into packets of given size. As soon as
a task exhausts its iterations, another packet is assigned to it.
* GUIDED: the iterations are divided into packets whose size decreases
exponentially. All the packets have a size greater than or equal to a
given value except for the last whose size may be less. As soon as a
task completes its iterations, another iteration package is assigned to
it.

*Case of an ordered execution*
[.text-justify]
* It is sometimes useful (debugging cases) to execute a loop in an
orderly fashion.
* The order of the iterations will then be identical to that
corresponding to a sequential execution.
* A reduction is an associative operation applied to a shared variable.
* The operation can be:
* arithmetic: +, --, *; +
logic: .AND., .OR., .EQV., .NEQV. ; +
an intrinsic function: MAX, MIN, IAND, IOR, IEOR.
* Each task calculates a partial result independently of the others.
They then sync to update the final result.

*Parallel sections*
[.text-justify]
* A section is a portion of code executed by one and only one task.
* Multiple portions of code can be defined by the user using the SECTION
directive within a SECTIONS construct.
* The goal is to be able to distribute the execution of several
independent portions of code on the different tasks.
* The NOWAIT clause is allowed at the end of the END SECTIONS construct
to remove the implicit synchronization barrier.
* All SECTION directives must appear within the lexical scope of the
SECTIONS construct.
* The clauses allowed in the SECTIONS directive are those we already
know:
* PRIVATE; FIRSTPRIVATE; LASTPRIVATE; REDUCTION.
* The PARALLEL SECTIONS directive is a merger of the PARALLEL and
SECTIONS directives with the union of their respective clauses.

*Exclusive execution*
[.text-justify]
Sometimes you want to exclude all tasks except one to execute certain
portions of code included in a parallel region.

To do this, OpenMP offers two directives SINGLE and MASTER.

Although the aim is the same, the behavior induced by these two
constructions remains quite different.

Parallel sections
[.text-justify]
* A section is a portion of code executed by one and only one task.
* Multiple portions of code can be defined by the user using the SECTION
directive within a SECTIONS construct.
* The goal is to be able to distribute the execution of several
independent portions of code on the different tasks.
* The NOWAIT clause is allowed at the end of the END SECTIONS construct
to remove the implicit synchronization barrier.

*Exclusive execution*
[.text-justify]
* Sometimes you want to exclude all tasks except one to execute certain
portions of code included in a parallel region.
* To do this, OpenMP offers two directives SINGLE and MASTER.
* Although the aim is the same, the behavior induced by these two
constructions remains quite different.

*SINGLE construction*
[.text-justify]
* The SINGLE construction allows a portion of code to be executed by one
and only one task without being able to specify which one.
* In general, it is the task which arrives first on the SINGLE
construction but it is not specified in the standard.
* All the tasks not executing the SINGLE region wait, at the end of the
END SINGLE construction, for the termination of the one responsible for
it, unless they have specified the NOWAIT clause.

*MASTER building*
[.text-justify]
* The MASTER construction allows a portion of code to be executed by the
master task alone.
* This construction does not admit any clauses.
* There is no synchronization barrier either at the beginning (MASTER)
or at the end of construction (END MASTER).

*Synchronizations*
[.text-justify]
Synchronization becomes necessary in the following situations:

[.text-justify]
{empty}1. to ensure that all concurrent tasks have reached the same
level of instruction in the program (global barrier);
[.text-justify]
{empty}2. to order the execution of all the concurrent tasks when these
must execute the same portion of code affecting one or more shared
variables whose consistency (in reading or in writing) in memory must be
guaranteed (mutual exclusion).
[.text-justify]
{empty}3. to synchronize at least two concurrent tasks among the set
(lock mechanism).

[.text-justify]
As we have already indicated, the absence of a NOWAIT clause means that
a global synchronization barrier is implicitly applied at the end of the
\openmp construction. But it is possible to explicitly impose a global
synchronization barrier thanks to the BARRIER directive.
[.text-justify]
The mutual exclusion mechanism (one task at a time) is found, for
example, in reduction operations (REDUCTION clause) or in the ordered
execution of a loop (DO ORDERED directive). For the same purpose, this
mechanism is also implemented in the ATOMIC and CRITICAL directives.
[.text-justify]
Finer synchronizations can be achieved either by setting up lock
mechanisms (this requires calling subroutines from the OpenMP library),
or by using the FLUSH directive.

*Barrier*
[.text-justify]
* The BARRIER directive synchronizes all concurrent tasks in a parallel
region.
* Each of the tasks waits until all the others have arrived at this
synchronization point to continue the execution of the program together.
* Atomic Update
* The ATOMIC directive ensures that a shared variable is read and
modified in memory by only one task at a time.
* Its effect is local to the statement immediately following the
directive.

*Critical regions*
[.text-justify]
* A critical region can be seen as a generalization of the ATOMIC
directive although the underlying mechanisms are distinct.
* The tasks execute this region in a non-deterministic order but one at
a time.
* A critical region is defined using the CRITICAL directive and applies
to a portion of code terminated by END CRITICAL.
* Its scope is dynamic.
* For performance reasons, it is not recommended to emulate an atomic
instruction by a critical region.

*FLUSH directive*
[.text-justify]
* It is useful in a parallel region to refresh the value of a shared
variable in global memory.
* It is all the more useful when the memory of a machine is
hierarchical.
* It can be used to implement a synchronization point mechanism between
tasks.

*Rules of good performance*
[.text-justify]
* Minimize the number of parallel regions in the code.
* Adapt the number of tasks requested to the size of the problem to be
treated in order to minimize the additional costs of task management by
the system.
* As much as possible, parallelize the outermost loop.
* Use the SCHEDULE(RUNTIME) clause to be able to dynamically change the
scheduling and the size of the iteration packets in a loop.
* The SINGLE directive and the NOWAIT clause can make it possible to
reduce the rendering time at the cost, most often, of an explicit
synchronization.
* The ATOMIC directive and the REDUCTION clause are more restrictive but
more powerful than the CRITICAL directive.
* Use the IF clause to implement conditional parallelization (eg on a
vector architecture, only parallelize a loop if its length is long
enough).
* Inter-task conflicts (of memory bank on a vector machine or of cache
faults on a scalar machine), can significantly degrade performance.

*OpenMP keywords*

[width="100%",cols="100%",]
|===
a|
== Directive (atomic, barrier, critical, flush, ordered, ….)

a|
An OpenMP executable directive applies to the succeeding structured
block or an OpenMP Construct. A “structured block” is a single statement
or a compound statement with a single entry at the top and a single exit
at the bottom.

    
    The *parallel* construction forms To team of threads and starts parallel
    execution.
    
    *#pragma comp parallel* _[clause[ [_ *,* _]clause] ...] new-line
    structured-block_
    
    _clause_ : *if(* _scalar- expression_ *)*
    
    *num_threads(* _integer-expression_ *) default(shared*  *none)
    private(* _list_ *) firstprivate(* _list_ *)*
    
    *shared(* _list_ *) copyin(* _list_ *) reduce(* _operator_ *:* _list_
    *)s*


a|
*loop* construction specifies that the iterations of loops will be
distributed among and executed by the encountering team of threads.


    *#pragma comp for* _[clause[[_ *,* _] clause] ... ] new-line for-loops_
    
    _clause_ : *private(* _list_ *)*
    
    *firstprivate(* _list_ *) lastprivate(* _list_ *) reduce(* _operator_
    *:* _list_ *) schedule(* _kind[, chunk_size]_ *) collapse(* _n_ *)*
    *ordered nowait*




a|
*sections* construct contains a set of structured blocks that are to be
distributed among and executed by the meeting team of threads.


    *#pragma comp sections* _[clause[[_ *,* _] clause] ...] new line_
    
    *{*
    
    _[_ *#pragma comp section* _new-line] structured-block_
    
    _[_ *#pragma comp section* _new-line structured-block ]_
    
    _clause_ : *private(* _list_ *)*
    
    *firstprivate(* _list_ *) 
    lastprivate(* _list_ *) reduce(* _operator_
    *:* _list_ *) nowait*

a|
*single* construction specifies that the associated structured block is
executed by only one of the threads in the team (not necessarily the
master thread), in the context of its implicit task.

    
    *#pragma comp single* _[clause[[_ *,* _] clause] ...] new-line
    structured-block_
    
    _clause_ : *private(* _list_ *)*
    
    *firstprivate(* _list_ *) copyprivate(* _list_ *) nowait*

a|
The combined parallel worksharing constructs are a shortcut for
specifying a parallel construct containing one worksharing construct and
no other statements. Allowed clauses are the union of the clauses
allowed for the *parallel* and worksharing constructs.


    *#pragma comp parallel for* _[clause[[_ *,* _] clause] ...] new-line
    for-loop_
    
    *#pragma comp parallel sections* _[clause[ [_ *,* _]clause] ...]
    new-line_
    
    *{*
    _[_ *#pragma comp section* _new-line] structured-block_
    
    _[_ *#pragma comp section* _new-line structured-block ]_
    
    _..._
    *#pragma comp task* _[clause[ [_ *,* _]clause] ...] new-line
    structured-block_
    _clause_ : *if(* _scalar- expression_ *)*
    
    === untied
    
    
    *default(shared  none) private(* _list_ *) firstprivate(* _list_ *)
    shared(* _list_ *)*
    
    *Master* construction specifies To structured block that is executed by
    the Master thread of the team. There is no implied barriers either on
    entry to, or exit from, the master construct.
    
    
    *#pragma comp Master* _new-line structured-block_

a|
*critical* construct restricts execution of the associated structured
block to a single thread at a time.


*#pragma comp critical* _[_ *(* _name_ *)* _] new-line structured-block_

    The *barriers* construction specifies year explicit barriers did the
    point did which the construct appears.
    
    *#pragma comp barriers* _new- line_
    
    The *taskwait* construction specifies To wait we the completion of child
    tasks generated since the beginning of the current task.
    
    *#pragma comp you asked* _new line_

a|
*atomic* construction ensures that To specific storage lease is updated
atomically, rather than exposing it to the possibility of multiple,
simultaneous writing threads.


    *#pragma comp atomic* _new-line expression-stmt_
    
    _stmt-expression_ : one of the following forms:
    
    _x binop_ *=* _expr x_ *++*
    
    *++* _x x_ *- -*
    
    *--x* ___


a|
*flush* construction execute the OpenMP flush operation, which makes a
thread's temporary view of memory consist with memories, and enforces an
order on the memory operations of the variables.


    *#pragma comp flush* _[_ *(* _list_ *)* _] new- line_


a|
The *ordered* construct specifies a structured block in a loop region
that will be executed in the order of the loop iterations. This
sequentializes and orders the code within an ordered region while
allowing code outside the region to run in parallel.


    *#pragma comp ordered* _new-line structured-block_
    
    
    a|
    *threadprivate* guideline specifies that variables are replicated, with
    each thread having its own copy.
    
    
    *#pragma comp threadprivate* _( list) new- line_

|===


[width="100%",cols="27%,73%",]
|===
a|
=== Parallel Execution
a|





|A Simple Parallel Loop a|
The loop iteration variable is private by default, so it is not
necessary to specify it explicitly in a private clause

    void simple(int n, float *a, float *b)
    {
    int i;
    *#pragma omp parallel for*
    for (i=1; i<n; i++) /* i is private by default */
    b[i] = (a[i] + a[i-1]) / 2.0;
    }
    
_    |The Parallel Construct a|
    The parallel construct can be used in coarse-grain parallel programs._
    
    void subdomain(float *x, int istart, int ipoints)
    {
    int i;
    for (i = 0; i < ipoints; i++)
    x[istart+i] = 123.456;
    }
    
    void sub(float *x, int npoints)
    {
    int iam, nt, ipoints, istart;
    *#pragma omp parallel default(shared) private(iam,nt,ipoints,istart)*
    {
    iam = omp_get_thread_num();
    nt = omp_get_num_threads();
    ipoints = npoints / nt; /* size of partition */
    istart = iam * ipoints; /* starting array index */
    if (iam == nt-1) /* last thread may do more */
    ipoints = npoints - istart;
    subdomain(x, istart, ipoints);
    }
    }
    
    main()
    {
    float array[10000]
    sub(array, 10000)
    return 0;
    }

|Controlling the Number of threads on Multiple Nesting Levels |The
OMP_NUM_THREADS environment variable to control the number of threads on
multiple nesting levels

|Interaction Between the num_threads Clause and omp_set_dynamic a|
The call to the omp_set_dynamic routine with argument 0 in C/C++,
disables the dynamic adjustment of the number of threads in OpenMP
implementations that support it.
    
    #include <omp.h>
    
    int main()
    {
    omp_set_dynamic(0);
    *#pragma omp parallel num_threads(10)*
    {
    /* do work here */
    }
    return 0;
    }

|The nowait Clause a|
If there are multiple independent loops within a parallel region, you
can use the nowait clause to avoid the implied barrier at the end of the
loop construct
    
    #include <math.h>
    
    void nowait_example(int n, int m, float *a, float *b, float *y, float *z)
    {
    int i;
    *#pragma omp parallel*
    {
    *#pragma omp for nowait*
    for (i=1; i<n; i++)
    b[i] = (a[i] + a[i-1]) / 2.0;
    *#pragma omp for nowait*
    for (i=0; i<m; i++)
    y[i] = sqrt(z[i]);
    }
    }

|The collapse Clause a|
The collapse clause is used since it is implicitly private. The collapse
clause associates one or more loops with the directive on which it
appears for the purpose of identifying the portion of the depth of the
canonical loop nest to which to apply the semantics of the directive.
The argument n speciﬁes the number of loops of the associated loop nest
to which to apply those semantics. On all directives on which the
collapse clause may appear, the eﬀect is as if a value of one was
speciﬁed for n if the collapse clause is not speciﬁed.

    void bar(float *a, int i, int j, int k);
    
    int kl, ku, ks, jl, ju, js, il, iu,is;
    
    void sub(float *a)
    {
    int i, j, k;
    *#pragma omp for collapse(2) private(i, k, j)*
    for (k=kl; k<=ku; k+=ks)
    for (j=jl; j<=ju; j+=js)
    for (i=il; i<=iu; i+=is)
    bar(a,i,j,k);
    }

|Linear Clause in Loop Constructs a|
The linear clause in a loop construct to allow the proper
parallelization of a loop that contains an induction variable (_j_). At
the end of the execution of the loop construct, the original variable
_j_ is updated with the value _N/2_ from the last iteration of the loop.

    #include <stdio.h>
    
    #define N 100
    
    int main(void)
    {
    float a[N], b[N/2];
    int i, j;
    for(i = 0;i<N;i++)
    a[i] = i+1;
    j=0
    *#pragma omp parallel*
    *#pragma omp for linear(j:1)*
    for(i=0;i<N;i+=2){
    b[j]= a[i] * 2.0f;
    j++;
    }
    
    printf"%d %f %f\n", j, b[0], b[j-1] );
    /* print out: 50 2.0 198.0 */
    return 0;
    }

|The firstprivate Clause and the sections Construct a|
The firstprivate clause is used to initialize the private copy of
section_count of each thread. The problem is that the section constructs
modify section_count, which breaks the independence of the section
constructs. When different threads execute each section, both sections
will print the value 1. When the same thread executes the two sections,
one section will print the value 1 and the other will print the value 2.
Since the order of execution of the two sections in this case is
unspecified, it is unspecified which section prints which value.

    #include <stdio.h>
    
    #define NT 4
    
    int main( ) {
    
    int section_count = 0;
    
    *omp_set_dynamic(0);*
    *omp_set_num_threads(NT);*
    *#pragma omp parallel*
    *#pragma omp sections firstprivate( section_count )*
    {
    
    *#pragma omp section*
    {
    section_count++;
    /* may print the number one or two */
    printf( "section_count %d\n", section_count );
    
    }
    
    *#pragma omp section*
    {
    section_count++;
    /* may print the number one or two */
    printf( "section_count %d\n", section_count );
    }
    
    }
    
    return 0;
    }

|The single Construct a|
Only one thread prints each of the progress messages. All other threads
will skip the single region and stop at the barrier at the end of the
single construct until all threads in the team have reached the barrier.
If other threads can proceed without waiting for the thread executing
the single region, a nowait clause can be specified, as is done in the
third single construct in this example. The user must not make any
assumptions as to which thread will execute a single region.

    #include <stdio.h>
    
    void work1() {}
    
    void work2() {}
    
    void single_example()
    
    
    *#pragma omp parallel*
    {
    *#pragma omp single*
    printf("Beginning work1.\n");
    work1();
    *#pragma omp single*
    printf("Finishing work1.\n");
    *#pragma omp single nowait*
    printf("Finished work1 and beginning work2.\n");
    work2();
    }
    }
    
    
    
    
    |The master Construct a|
    #include <stdio.h>
    
    extern float average(float,float,float);
    void master_example( float* x, float* xold, int n, float tol )
    {
    int c, i, toobig;
    float error, y;
    c = 0;
    
    #*pragma omp parallel*
    {
    do {
    *#pragma omp for private(i)*
    for( i = 1; i < n-1; ++i ){
    xold[i] = x[i];
    }
    
    *#pragma omp single*
    {
    toobig = 0;
    }
    
    *#pragma omp for private(i,y,error) reduction(+:toobig)*
    for(i=1; i<n-1;++i){
    y = x[i];
    x[i] = average( xold[i-1], x[i], xold[i+1] );
    error = y - x[i];
    if( error > tol or error < -tol ) ++toobig;
    }
    
    *#pragma omp master*
    {
    ++c;
    printf( "iteration %d, toobig=%d\n", c, toobig );
    }
    } while( toobig > 0 );
    }
    }



|Parrallel Random Access Iterator Loop a|
    #include <vector>
    
    void iterator_example()
    
    {
    std::vector<int> vec(23);
    std::vector<int>::iterator it;
    
    *#pragma omp parallel for default(none) shared(vec)*
    for (it = vec.begin(); it < vec.end(); it++)
    
    {
    // do work with *it //
    }
    }

|The omp_set_dynamic and omp_set_num_threads Routines a|
Some programs rely on a fixed, prespecified number of threads to execute
correctly. Because the default setting for the dynamic adjustment of the
number of threads is implementation defined, such programs can choose to
turn off the dynamic threads capability and set the number of threads
explicitly to ensure portability.

    #include <omp.h>
    
    #include <stdlib.h>
    
    void do_by_16(float *x, int iam, int ipoints) {}
    
    void dynthreads(float *x, int npoints)
    {
    int iam, ipoints;
    *omp_set_dynamic(0);*
    *omp_set_num_threads(16);*
    *#pragma omp parallel shared(x, npoints) private(iam, ipoints)*
    {
    if (omp_get_num_threads() != 16) abort();
    iam = omp_get_thread_num();
    ipoints = npoints/16;
    do_by_16(x, iam, ipoints);
    }
    }

|===

[width="100%",cols="26%,74%",]
|===
a|
=== *Clauses: Data Sharing attribute*

|
_Data sharing attribute clauses apply only to variables whose names are
visible in the construct on which the clause appears. Not all of the
clauses are valid on all directives. The set of clauses that is valid we
To particular guideline is described with the directive. Most of the
clauses accept a comma-separated list of list items. All list items
appearing in a clause must be visible._ 



a|
default(shared none);

a|
Controls the default data sharing attributes of variables that are
referenced in a *parallel* or *task* construct.


a|

*shared(* _list_ *);*


a|

Declared one gold more list items to be shared by tasks generated by a
*parallel* or *task* construct.

a|

*private(* _list_ *);*


a|

Declared one or more list items to be private to a task.


a|

*firstprivate(* _list_ *);*


a|

Declared one gold more list items to be private to To task, and
initialize each of them with the value that the corresponding original
item has when the construct is encountered.


a|

*lastprivate(* _list_ *);*


a|

Declares one or more list items to be private to an implicit task, and
causes the corresponding original item to be updated after the end of
the region.


a|

*reduce(* _operator_ *:* _list_ *);*


a|

Declares accumulation into the list items using the indicated
associative operator. Accumulation occurs into To private copy for each
list item which is then combined with the original item.


|===

[width="100%",cols="24%,76%",]
|===
a|
=== Clauses: Data copying

|

_Thesis clauses support the copying of data values from private gold
thread- private variables on one implicit task or thread to the
corresponding variables on other implicit tasks or threads in the team._


a|

*copyin(* _list_ *);*


a|

Copies the value of the master thread's _threadprivate_ variable to the
_threadprivate_ variable of each other member of the team executing the
*parallel* region.


a|

*copyprivate(* _list_ *);*


a|

Broadcasts a value from the data environment of one implicit task to the
data environments of the other implied tasks belonging to the *parallel*
region.


|===

[width="100%",cols="39%,61%",]
|===
a|
=== Execution Environment Routines Function



|_Execution environment routines affect and monitor threads, processors,
and the parallel environment. Lock routines support synchronization with
OpenMP locks. Timing routines support a portable wall clock timer.
prototypes for the runtime library routines are defined in the queue
“omp.h”._ |

a|
a|
void omp_set_num_threads(int* _num_threads_ *);

|Affects the number of threads used for subsequent *parallel* regions
that do not specify To *num_threads* clause.

a|
int omp_get_num_threads(void);

|Returns the nusmber of threads in the current team.

a|
int omp_get_max_threads(void);

|Returns maximum number of threads that could be used to form To new
team using a “parallel” construct without has “num_threads” clause.

a|
int omp_get_thread_num(void);

|Returns tea ID of the meeting thread where ID rows from zero to the
size of the team minus 1.

a|
int omp_get_num_procs(void);

|Returns the number of processors available to the program.

a|
int omp_in_parallel(void);

|Returns _true_ if the call to the routine is enclosed by an active
*parallel* region; otherwise, it returns _false_ .

a|
void omp_set_dynamic(int* _dynamic_threads_ *);


|Enables gold disables dynamic adjustments of the number of threads
available.

a|
int omp_get_dynamic(void);

|Returns the value of the _dyn-var_ internal control variable (ICV),
determining whether dynamic adjustments of the number of threads is
enabled or disabled.

a|
void omp_set_nested(int _nested_ );

|Enables gold disables nested parallelism, by setting the _nest-var_
ICV.

a|
int omp_get_nested(void);

|Returns the value of the _nest-var_ LCI, which determined if nested
parallelism is enabled or disabled.

a|

void omp_set_schedule(omp_sched_t* _kind_ *, int* _modify_ *);


|Affects the schedule that is applied when *run-time* is used as
schedule kind, by setting the value of the _run-sched-var_ ICV.

a|
void omp_get_schedule (omp_sched_t *kind, int *edit)s;

|Returns the schedule applied when *run-time* schedule is used.

a|
int omp_get_thread_limit(void)* 

|Returns the maximum number of OpenMP
threads available to the program.

a|
int omp_get_thread_limit(void)* |Returns the maximum number of OpenMP
threads available to the program.

a|

void omp_set_max_active_levels(int* _max_levels_ *);* |Limits the
number of nested active *parallel* regions, by setting the
_max-active-levels-var_ ICV.

a|
int omp_get_max_active_levels(void);

|Returns tea value of tea _max-activelevels-var LCI_ , which determines
the maximum number of nested active *parallel* regions.

a|
int omp_get_level(void);

|Returns tea number of nested *parallel* regions enclosing tea task that
contains the call.

a|
int omp_get_ancestor_thread_num(int _level_ );

|Returns, for To given nested level of tea current thread, tea thread
number of the ancestor or the current thread.

a|
int omp_get_team_size(int _level_ );

|Returns, for To given nested level of tea current thread, tea size of
the thread team to which the ancestor or the current thread belongs.

a|
int omp_get_active_level(void);

|Returns tea number of nested, active *parallel* regions enclosing the
task that contains the call.
|===



[width="100%",cols="41%,59%",]
|===
a|
=== Lock Routines

|

a|
void omp_init_lock(omp_lock_t * _lock_ );


*void omp_init_nest_lock(omp_nest_lock_t ** _lock_ *);*


|Routines initialize year OpenMP lock.

a|
void omp_destroy_lock(omp_lock_t * _lock_ );


*void omp_destroy_nest_lock(omp_nest_lock_t ** _lock_ *);*


|Routines ensure that the OpenMP lock is uninitialized.

a|
void omp_set_lock(omp_lock_t * _lock_ );


*void omp_set_nest_lock(omp_nest_lock_t ** _lock_ *);*


|Routines provide To means of setting year OpenMP lock.

a|
void omp_unset_lock(omp_lock_t * _lock_ );


*void omp_unset_nest_lock(omp_nest_lock_t ** _lock_ *);*

|Routines provide To means of setting year OpenMP lock.

a|
int omp_test_lock(omp_lock_t * _lock_ );


*int omp_test_nest_lock(omp_nest_lock_t ** _lock_ *);*


|Routines attempt to set year OpenMP lock aim do not suspend execution
of the task executing the routine.
|===

[width="100%",cols="41%,59%",]
|===
a|
=== Timing Routines

|
a|
double omp_get_wtime(void);

|Returns elapsed wall clock time in seconds.
a|
double omp_get_wtick(void);

|Returns the precision of the timer used by *omp_get_wtime* .
|===

[width="100%",cols="35%,65%",]
|===
a|
=== Environment Variables



|_Environment variable names are upper case, and the values assigned to
them are box insensitive and May have leading and trailing white space._


a|

OMP_SCHEDULE* _type_ *[,* _chunk_ *]


|Sets the _run-sched-var_ ICV for the runtime schedule type and chunk
size. Valid OpenMP schedule types are *static* _,_ *dynamic* _,_
*guided* , or *auto* . _Chunk_ is a positive integer.

a|OMP_NUM_THREADS _number_

|Sets the _nthreads-var_ LCI for tea number of threads to worn for
*parallel* regions.

a|

*OMP_DYNAMIC* _dynamic_

|Sets the _dyn-var_ ICV _for_ the dynamic adjustment of threads to use
for *parallel* regions. Valid values for _dynamic_ are *true* gold
*false* .

a|

*OMP_NESTED* _nested_


|Sets the _nest-var_ LCI to enable gold to disable nested parallelism.
Valid values for _nested_ are true or false.

a|

*OMP_STACKSIZE* _size_

|Sets the _stacksize-var_ ICV that specifies the size of the stack for
threads created by the OpenMP implementation. Valid values for _size_ (a
positive integer) are _size_ , _size_ *B* , _size_ *K* , _size_ *M* ,
_size_ *G.* _ Yew units *B* , *K* , *M* or *G* are not specified, size
is measured in kilobytes ( *K* ).

a|

*OMP_WAIT_POLICY* _policy_


|Sets the _wait-policy-var_ ICV that controls the desired behavior of
waiting threads. Valid values for _policy_ are *active* (waiting threads
consume processor cycles while waiting) and *passive* .

a|

*OMP_MAX_ACTIVE_LEVELS* _levels_

|Sets tea _max-active-levels-var_ LCI that controls the maximum number
of nested active *parallel* regions.

a|

*OMP_THREAD_LIMIT* _limit_


|Sets tea _thread-limit-var_ LCI that controls the maximum number of
threads participating in the OpenMP program.
|===

[width="100%",cols="35%,65%",options="header",]
|===
a|
Operators legally allowed in at discount

|
a|

*Operator*


a|

*Initialization value*


a|

+


|0
a|

*


|1
a|

-


|0
a|

&


a|

~0


a|

|


|0
a|

^


|0
a|

&&

|1
a|

||


|0
|===

[width="100%",cols="22%,78%",]
|===
|*Schedule types for the loop construct* |

a|

*static*


|Iterations are divided into chunks of size _chunk_size_ , and the
chunks are assigned to the threads in the team in a round-robin fashion
in the order of the thread number.

a|

*dynamic*


|Each thread execute To chunk of iterations, then requests another
chunk, until no chunks remain to be distributed.

a|

*guided*


|Each thread execute To chunk of iterations, then requests another
chunk, until no chunks remain to be assigned. The chunk sizes start
large and shrink to the indicated _chunk_size_ as chunks are scheduled.

a|

*car*


|The decision regarding scheduling is delegated to the compiler and/or
runtime system.

a|

*run-time*


|The schedule and chunk size are taken from the run-sched-var ICV.
|===

*2.3 Hybrid MPI and OpenMP*

Hybrid application programs using MPI + OpenMP are now commonplace on
large HPC systems. There are basically two main motivations for this
combination of programming models:

{empty}1. Reduced memory footprint, both in the application and in the
MPI library (eg communication buffers).

{empty}2. Improved performance, especially at high core counts where
pure MPI scalability runs out.

A common hybrid approach

image::image9.png[xref=#fragment9,width=307,height=155]



* From dequential code, alongside MPI first, then try adding OpenMP
* From MPI code, add OpenMP
* From OpenMP code, treat as serial code
* The simplest and least error-prone method is to use MPI outside the
parallel region and allow only the master thread to communicate between
MPI tasks.
* Could use MPI in parallel region with thread-safe MPI.

image::image10.png[xref=#fragment10,width=264,height=166]

