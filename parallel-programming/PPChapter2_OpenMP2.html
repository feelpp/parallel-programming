<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Guide into OpenMP :: Parallel Programming</title>
    <link rel="canonical" href="https://feelpp.github.io/parallel-programming/parallel-programming/PPChapter2_OpenMP2.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../_/css/site.css">
<link rel="icon" href="../_/img/favicon.ico" type="image/x-icon">
<script>!function(l,p){if(l.protocol!==p&&l.host=="docs.antora.org"){l.protocol=p}else if(/\.gitlab\.io$/.test(l.host)){l.replace(p+"//docs.antora.org"+l.pathname.substr(l.pathname.indexOf("/",1))+l.search+l.hash)}}(location,"https:")</script>

<script src="../_/js/vendor/tabs-block-extension.js"></script>
<script src="../_/js/vendor/tabs-block-behavior.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },

  TeX: {
      Macros: {
      bold: ["{\\bf #1}",1],
      calTh: "{\\mathcal{T}_h}",
      card: ["{\\operatorname{card}(#1)}",1],
      card: ["{\\operatorname{card}(#1)}",1],
      Ck: ["{\\mathcal{C}^{#1}}",1],
      deformt: ["{\\mathbf{\\varepsilon(#1)}}",1],
      diam: "{\\operatorname{diam}}",
      dim: ["{\\operatorname{dim}(#1)}",1],
      disp: ["{\\mathbf{#1}}",1],
      domain: "{\\Omega}",
      ds: "",
      essinf: "{\\operatorname{ess}\\, \\operatorname{inf}}",
      F:"{\\mathcal{F}}",
      geo: "{\\mathrm{geo}}",
      Ich: ["{\\mathcal{I}^{#1}_{c,h}#2}",2],
      Id: "{\\mathcal{I}}",
      Ilag: ["{\\mathcal{I}^{\\mathrm{lag}}_{#1}}",1],
      jump: ["{[\\![ #1 ]\\!]}",1],
      n:"{\\mathbf{n}}",
      Ne: "{N_{\\mathrm{e}}}",
      Next: "{\\mathrm{n}}",
      nf: "{n_f}",
      ngeo: "{n_{\\mathrm{geo}}}",
      Nma: "{N_{\\mathrm{ma}}}",
      NN: "{\\mathbb N}",
      Nno: "{N_{\\mathrm{no}}}",
      Nso: "{N_{\\mathrm{so}}}",
      opdim: "{\\operatorname{dim}}",
      p: "{\\mathrm{p}}",
      P:"{\\mathcal{P}}",
      Pch: ["{P^{#1}_{c,h}}",1],
      Pcho: ["{P^{#1}_{c,h,0}}",1],
      Pk: ["{\\mathcal{P}^{#1}}",1],
      poly: ["{\\mathbb{#1}",1],
      poly: ["{\\mathbb{#1}}",1],
      prect: ["{\\left\\(#1\\right\\)}",1],
      q:"{\\mathbf{q}}",
      Qch: ["{Q^{#1}_{c,h}}",1],
      Qk: ["{\\mathcal{Q}^{#1}}",1],
      R: ["{\\mathbb{R}^{#1}}",1],
      RR: "{\\mathbb R}",
      set: ["{\\left\\{#1\\right\\}}",1],
      stresst: ["{\\mathbf{\\sigma(#1)}}",1],
      T:"{\\mathcal{T}}",
      tr: "{\\operatorname{tr}}",
      v:"{\\mathbf{v}}",
      vertiii: ["\\left\\vert\\kern-0.25ex\\left\\vert\\kern-0.25ex\\left\\vert #1 \\right\\vert\\kern-0.25ex\\right\\vert\\kern-0.25ex\\right\\vert",1]
  },
  extensions: ["mhchem.js"] 
  }
});
</script>
<!--<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script>-->
<!-- <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_CHTML'></script>
<!--<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.0/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>-->

<!--<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" integrity="sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js" integrity="sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq" crossorigin="anonymous"></script>-->
<script>var uiRootPath = '../_'</script>

  </head>
  <body class="article">
<header class="header">
    <nav class="navbar navbar-expand-sm bg-dark navbar-dark navbar-template-project" style="border-top: 4px solid #9E9E9E">
        <div class="navbar-brand">
            <div class="navbar-item feelpp-logo">
                <a href="https://feelpp.github.io/parallel-programming">Parallel Programming</a>
            </div>
            <button class="navbar-burger" data-target="topbar-nav">
                <span></span>
                <span></span>
                <span></span>
            </button>
        </div>

        <div id="topbar-nav" class="navbar-menu">
            <div class="navbar-end">
                <div class="navbar-item">
                    <a href="https://docs.feelpp.org/">Documentation Reference</a>
                </div>
                <div class="navbar-item has-dropdown is-hoverable download-item">
                    <div class="navbar-item"><a href="https://docs.feelpp.org/user/latest/install/index.html" class="download-btn">Get Feel++</a></div>
                </div>
                <div class="navbar-item">
                    <a class="navbar-brand"  href="https://www.cemosis.fr">
                        <img class="cemosis-logo"  src="../_/img/cemosis-logo.svg" alt="Cemosis logo"/>
                    </a>
                </div>
            </div>
        </div>
    </nav>
</header>
<div class="body">
<a href="#" class="menu-expand-toggle"></a>
<div class="nav-container" data-component="parallel-programming" data-version="">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="index.html">Main</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="index.html">Introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter1_CPU.html">CPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter1_GPU.html">GPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter1_GPGPU.html">GPGPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter1_TPU.html">TPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter1_SIMD.html">SIMD Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter2_MPI.html">MPI (Message Passing Interface)</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter2_MPI_Boost.html">MPI Boost</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter2_OpenMP.html">OpenMP (Open Multi-Processing)</a>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="PPChapter2_OpenMP2.html">OpenMP more information</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter2_Hybrid.html">Hybrid MPI with OpenMP</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter3.html">StarPU</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="PPChapter4.html">Specx</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Main</span>
    <span class="version"></span>
  </div>
  <ul class="components">
      <li class="component">
        <a class="title" href="../feelpp-antora-ui/index.html">Antora Feel++ UI</a>
      </li>
      <li class="component is-current">
        <a class="title" href="index.html">Main</a>
      </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
  <button class="nav-toggle"></button>
    <a href="index.html" class="home-link"></a>
  <nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="index.html">Main</a></li>
    <li><a href="PPChapter2_OpenMP2.html">OpenMP more information</a></li>
  </ul>
</nav>

  
    <div class="edit-this-page"><a href="https://github.com/feelpp/parallel-programming/edit/lem/docs/modules/ROOT/pages/PPChapter2_OpenMP2.adoc">Edit this Page</a></div>
  
  <div class="page-downloads">
  <span class="label">Download as</span>
  <ul class="download-options">
    <li>
      <a onclick="print(this)" href="#" data-toggle="tooltip" data-placement="left" title="Print to PDF"
         class="pdf-download">
        <img class="pdf-file-icon icon" src="../_/img/pdf.svg"/> .pdf
      </a>
    </li>
  </ul>
</div>
</div>

  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="3">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Guide into OpenMP</h1>
<div class="sect1">
<h2 id="_introduction"><a class="anchor" href="#_introduction"></a>1. Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>OpenMP consists of a set of compiler #pragmas that control how the program works. The pragmas are designed so that even if the compiler does not support them, the program will still yield correct behavior, but without any parallelism.</p>
</div>
<div class="paragraph">
<p>Here are two simple example programs demonstrating OpenMP.</p>
</div>
<div class="paragraph">
<p>You can compile them like this:</p>
</div>
<div class="paragraph">
<p>g++ tmp.cpp -fopenmp</p>
</div>
<div class="sect2">
<h3 id="_example_initializing_a_table_in_parallel_multiple_threads"><a class="anchor" href="#_example_initializing_a_table_in_parallel_multiple_threads"></a>1.1. Example: Initializing a table in parallel (multiple threads)</h3>
<div class="paragraph">
<p>This code divides the table initialization into multiple threads, which are run simultaneously. Each thread initializes a portion of the table.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#include &lt;cmath&gt;
int main()
{
    const int size = 256;
    double sinTable[size];
    #pragma omp parallel for
    for(int n=0; n&lt;size; ++n)
        sinTable[n] = std::sin(2 * M_PI * n / size);
}</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_example_initializing_a_table_in_parallel_single_thread_simd"><a class="anchor" href="#_example_initializing_a_table_in_parallel_single_thread_simd"></a>1.2. Example: Initializing a table in parallel (single thread, SIMD)</h3>
<div class="paragraph">
<p>This version requires compiler support for at least OpenMP 4.0, and the use of a parallel floating point library such as AMD ACML or Intel SVML (which can be used in GCC with e.g. ‑mveclibabi=svml).</p>
</div>
<div class="paragraph">
<p>#include &lt;cmath&gt;
    int main()
    {
        const int size = 256;
        double sinTable [ size ];
        #pragma omp simd
        for(int n=0; n&lt;size; ++n)
            sinTable[n] = std::sin(2 * M_PI * n / size);
    }</p>
</div>
</div>
<div class="sect2">
<h3 id="_example_initializing_a_table_in_parallel_multiple_threads_on_another_device"><a class="anchor" href="#_example_initializing_a_table_in_parallel_multiple_threads_on_another_device"></a>1.3. Example: Initializing a table in parallel (multiple threads on another device)</h3>
<div class="paragraph">
<p>OpenMP 4.0 added support for offloading code to different devices, such as a GPU. Therefore there can be three layers of parallelism in a single program: Single thread processing multiple data; multiple threads running simultaneously; and multiple devices running same program simultaneously.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#include &lt;cmath&gt;
int main()
{
    const int size = 256;
    double sinTable[size];
    #pragma omp target teams distribute parallel for map
    for(int n=0; n&lt;size; ++n)
        sinTable[n] = std::sin(2 * M_PI * n / size);
    // the table is now initialized
}</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_example_calculating_the_mandelbrot_fractal_in_parallel_host_computer"><a class="anchor" href="#_example_calculating_the_mandelbrot_fractal_in_parallel_host_computer"></a>1.4. Example: Calculating the Mandelbrot fractal in parallel (host computer)</h3>
<div class="paragraph">
<p>This program calculates the classic Mandelbrot fractal at a low resolution and renders it with ASCII characters, calculating multiple pixels in parallel.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#include &lt;complex&gt;
#include &lt;cstdio&gt;</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>typedef std::complex&lt;double&gt; complex;
int MandelbrotCalculate(complex c, int maxiter)
{
    complex z = c;
    int n=0;
    for(; n&lt;maxiter; ++n)
    {
        if( std::abs(z) &gt;= 2.0) break;
        z = z*z + c;
    }
    return n;
}</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>int main()
{
    const int width = 78, height = 44, num_pixels = width*height;
    const complex center(-.7, 0), span(2.7, -(4/3.0)*2.7*height/width);
    const complex begin = center-span/2.0;//, end = center+span/2.0;
    const int maxiter = 100000;
    #pragma omp parallel for ordered schedule(dynamic)
    for(int pix=0; pix&lt;num_pixels; ++pix)
    {
        const int x = pix%width, y = pix/width;
        complex c = begin + complex(x * span.real() / (width +1.0),
        y * span.imag() / (height+1.0));
        int n = MandelbrotCalculate(c, maxiter);
        if(n == maxiter) n = 0;</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>        #pragma omp ordered
        {
        char c = ' ';
        if(n &gt; 0)
        {
            static const char charset[] = ".,c8M@jawrpogOQEPGJ";
            c = charset[n % (sizeof(charset)-1)];
        }
        std::putchar(c);
        if(x+1 == width) std::puts("|");
        }
    }
}</pre>
</div>
</div>
<div class="paragraph">
<p>This program can be improved in many different ways, but it is left simple for the sake of an introductory example.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_syntax"><a class="anchor" href="#_the_syntax"></a>2. The syntax</h2>
<div class="sectionbody">
<div class="paragraph">
<p>All OpenMP constructs in C and C++ are indicated with a #pragma omp followed by parameters, ending in a newline. The pragma usually applies only into the statement immediately following it, except for the barrier and flush commands, which do not have associated statements.</p>
</div>
<div class="sect2">
<h3 id="_the_parallel_construct"><a class="anchor" href="#_the_parallel_construct"></a>2.1. The parallel construct</h3>
<div class="paragraph">
<p>The parallel construct starts a parallel block. It creates a <em>team</em> of N threads (where N is determined at runtime, usually from the number of CPU cores, but may be affected by a few things), all of which execute the next statement (or the next block, if the statement is a {…} -enclosure). After the statement, the threads join back into one.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp parallel
{
    // Code inside this region runs in parallel.
    printf("Hello!\n");
}</pre>
</div>
</div>
<div class="paragraph">
<p>This code creates a team of threads, and each thread executes the same code. It prints the text "Hello!" followed by a newline, as many times as there are threads in the team created. For a dual-core system, it will output the text twice. (Note: It may also output something like "HeHlellolo", depending on system, because the printing happens in parallel.) At the }, the threads are joined back into one, as if in non-threaded program.</p>
</div>
<div class="paragraph">
<p>Internally, GCC implements this by creating a magic function and moving the associated code into that function, so that all the variables declared within that block become local variables of that function (and thus, locals to each thread).<br>
ICC, on the other hand, uses a mechanism resembling fork(), and does not create a magic function. Both implementations are, of course, valid, and semantically identical.</p>
</div>
<div class="paragraph">
<p>Variables shared from the context are handled transparently, sometimes by passing a reference and sometimes by using register variables which are flushed at the end of the parallel block (or whenever a flush is executed).</p>
</div>
<div class="sect3">
<h4 id="_parallelism_conditionality_clause_if"><a class="anchor" href="#_parallelism_conditionality_clause_if"></a>2.1.1. Parallelism conditionality clause: if</h4>
<div class="paragraph">
<p>The parallelism can be made <em>conditional</em> by including a if clause in the parallel command, such as:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>extern int parallelism_enabled;
#pragma omp parallel for if(parallelism_enabled)
for(int c=0; c&lt;n; ++c)
handle(c);</pre>
</div>
</div>
<div class="paragraph">
<p>In this case, if parallelism_enabled evaluates to a zero value, the number of threads in the team that processes the for loop will always be exactly one.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_loop_construct_for"><a class="anchor" href="#_loop_construct_for"></a>2.2. Loop construct: for</h3>
<div class="paragraph">
<p>The for construct splits the for-loop so that each thread in the current team handles a different portion of the loop.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp for
for(int n=0; n&lt;10; ++n)
{
    printf(" %d", n);
}
printf(".\n");</pre>
</div>
</div>
<div class="paragraph">
<p>This loop will output each number from 0…9 once. However, it may do it in arbitrary order. It may output, for example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>0 5 6 7 1 8 2 3 4 9.</pre>
</div>
</div>
<div class="paragraph">
<p>Internally, the above loop becomes into code equivalent to this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int this_thread = omp_get_thread_num(), num_threads = omp_get_num_threads();
int my_start = (this_thread ) * 10 / num_threads;
int my_end = (this_thread+1) * 10 / num_threads;
for(int n=my_start; n&lt;my_end; ++n) printf(" %d", n);</pre>
</div>
</div>
<div class="paragraph">
<p>So each thread gets a different section of the loop, and they execute their own sections in parallel.</p>
</div>
<div class="paragraph">
<p>Note: #pragma omp for only delegates portions of the loop for different threads in the <em>current team</em>. A <em>team</em> is the group of threads executing the program. At program start, the team consists only of a single member: the master thread that runs the program.</p>
</div>
<div class="paragraph">
<p>To create a new team of threads, you need to specify the parallel keyword. It can be specified in the surrounding context:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp parallel
{
    #pragma omp for
    for(int n=0; n&lt;10; ++n) printf(" %d", n);
}</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>printf(".\n");</pre>
</div>
</div>
<div class="paragraph">
<p>Equivalent shorthand is to specify it in the pragma itself, as #pragma omp parallel for:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp parallel for
for(int n=0; n&lt;10; ++n) printf(" %d", n);
printf(".\n");</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>You can explicitly specify the number of threads to be created in the team, using the num_threads attribute:</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp parallel num_threads ( 3 )
{
// This code will be executed by three threads.
// Chunks of this loop will be divided amongst
// the (three) threads of the current team.
    #pragma omp for
    for(int n=0; n&lt;10; ++n) printf(" %d", n);
}</pre>
</div>
</div>
<div class="paragraph">
<p>Note that OpenMP also works for C. However, in C, you need to set explicitly the loop variable as private, because C does not allow declaring it in the loop body:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int n;
#pragma omp for private(n)
for(n=0; n&lt;10; ++n) printf(" %d", n);
printf(".\n");</pre>
</div>
</div>
<div class="paragraph">
<p>See the "private and shared clauses" section for details.</p>
</div>
<div class="paragraph">
<p>In OpenMP 2.5, the iteration variable in for must be a signed integer variable type. In OpenMP 3.0, it may also be an unsigned integer variable type, a pointer type or a constant-time random access iterator type. In the latter case, std::distance() will be used to determine the number of loop iterations.</p>
</div>
<div class="sect3">
<h4 id="_what_are_parallel_for_and_a_team"><a class="anchor" href="#_what_are_parallel_for_and_a_team"></a>2.2.1. What are: parallel, for and a team</h4>
<div class="paragraph">
<p>The difference between parallel, parallel for and for is as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A team is the group of threads that execute currently.</p>
<div class="ulist">
<ul>
<li>
<p>At the program beginning, the team consists of a single thread.</p>
</li>
<li>
<p>A parallel construct splits the current thread into <em>a new team</em> of threads for the duration of the next block/statement, after which the team merges back into one.</p>
</li>
</ul>
</div>
</li>
<li>
<p>for divides the work of the for-loop among the threads of the <em>current team</em>. It does not create threads, it only divides the work amongst the threads of the currently executing team.</p>
</li>
<li>
<p>parallel for is a shorthand for two commands at once: parallel and for. Parallel creates a new team, and for splits that team to handle different portions of the loop.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>If your program never contains a parallel construct, there is never more than one thread; the master thread that starts the program and runs it, as in non-threading programs.</p>
</div>
</div>
<div class="sect3">
<h4 id="_scheduling"><a class="anchor" href="#_scheduling"></a>2.2.2. Scheduling</h4>
<div class="paragraph">
<p>The scheduling algorithm for the for-loop can explicitly controlled.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp for schedule(static)
for(int n=0; n&lt;10; ++n) printf(" %d", n);
printf(".\n");</pre>
</div>
</div>
<div class="paragraph">
<p>There are five scheduling types: static, dynamic, guided, auto, and (since OpenMP 4.0) runtime. In addition, there are three scheduling modifiers (since OpenMP 4.5): monotonic, nonmonotonic, and simd.</p>
</div>
<div class="paragraph">
<p>static is the default schedule as shown above. Upon entering the loop, each thread independently decides which chunk of the loop they will process.</p>
</div>
<div class="paragraph">
<p>There is also the dynamic schedule:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp for schedule(dynamic)
for(int n=0; n&lt;10; ++n) printf(" %d", n);
printf(".\n");</pre>
</div>
</div>
<div class="paragraph">
<p>In the dynamic schedule, there is no predictable order in which the loop items are assigned to different threads. Each thread asks the OpenMP runtime library for an iteration number, then handles it, then asks for next, and so on. This is most useful when used in conjunction with the ordered clause, or when the different iterations in the loop may take different time to execute.</p>
</div>
<div class="paragraph">
<p>The chunk size can also be specified to lessen the number of calls to the runtime library:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp for schedule(dynamic, 3)
for(int n=0; n&lt;10; ++n) printf(" %d", n);
printf(".\n");</pre>
</div>
</div>
<div class="paragraph">
<p>In this example, each thread asks for an iteration number, executes 3 iterations of the loop, then asks for another, and so on. The last chunk may be smaller than 3, though.</p>
</div>
<div class="paragraph">
<p>Internally, the loop above becomes into code equivalent to this (illustration only, do not write code like this):</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int a,b;
if(GOMP_loop_dynamic_start(0,10,1, 3, &amp;a,&amp;b))
{
    do {
    for(int n=a; n&lt;b; ++n) printf(" %d", n);
    } while(GOMP_loop_dynamic_next(&amp;a,&amp;b));
}</pre>
</div>
</div>
<div class="paragraph">
<p>The "runtime" option means the runtime library chooses one of the scheduling options at runtime at the compiler library&#8217;s discretion.</p>
</div>
<div class="paragraph">
<p>A scheduling modifier can be added to the clause, e.g.: #pragma omp for schedule(nonmonotonic:dynamic<br>
The modifiers are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>monotonic: Each thread executes chunks in an increasing iteration order.</p>
</li>
<li>
<p>nonmonotonic: Each thread executes chunks in an unspecified order.</p>
</li>
<li>
<p>simd: If the loop is a simd loop, this controls the chunk size for scheduling in a manner that is optimal for the hardware limitations according to how the compiler decides. This modifier is ignored for non-SIMD loops.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_the_ordered_clause"><a class="anchor" href="#_the_ordered_clause"></a>2.2.3. The ordered clause</h4>
<div class="paragraph">
<p>The order in which the loop iterations are executed is unspecified, and depends on runtime conditions.</p>
</div>
<div class="paragraph">
<p>However, it is possible to force that certain events within the loop happen in a predicted order, using the ordered clause.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp for ordered schedule(dynamic)</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>for(int n=0; n&lt;100; ++n)
{
    files[n].compress();
    #pragma omp ordered
    send(files[n]);
}</pre>
</div>
</div>
<div class="paragraph">
<p>This loop "compresses" 100 files with some files being compressed in parallel, but ensures that the files are "sent" in a strictly sequential order.</p>
</div>
<div class="paragraph">
<p>If the thread assigned to compress file 7 is done but the file 6 has not yet been sent, the thread will wait before sending, and before starting to compress another file. The ordered clause in the loop guarantees that there always exists one thread that is handling the lowest-numbered unhandled task.</p>
</div>
<div class="paragraph">
<p>Each file is compressed and sent exactly once, but the compression may happen in parallel.</p>
</div>
<div class="paragraph">
<p>There may only be one ordered block per an ordered loop, no less and no more. In addition, the enclosing for construct must contain the ordered clause.</p>
</div>
<div class="paragraph">
<p>OpenMP 4.5 added some modifiers and clauses to the ordered construct.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>#pragma omp ordered threads means the same as #pragma omp ordered. It means the threads executing the loop execute the ordered regions sequentially in the order of loop iterations.</p>
</li>
<li>
<p>#pragma omp ordered simd can only be used in a for simd loop.</p>
</li>
<li>
<p>#pragma omp ordered depend(source) and #pragma omp ordered depend(vectorvariable) also exist.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_the_collapse_clause"><a class="anchor" href="#_the_collapse_clause"></a>2.2.4. The collapse clause</h4>
<div class="paragraph">
<p>When you have nested loops, you can use the collapse clause to apply the threading to multiple nested iterations.</p>
</div>
<div class="paragraph">
<p>Example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp parallel for collapse(2)</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>for(int y=0; y&lt;25; ++y)
    for(int x=0; x&lt;80; ++x)
    {
        tick(x,y);
    }</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_the_reduction_clause"><a class="anchor" href="#_the_reduction_clause"></a>2.2.5. The reduction clause</h4>
<div class="paragraph">
<p>The reduction clause is a special directive that instructs the compiler to generate code that accumulates values from different loop iterations together in a certain manner. It is discussed in a separate chapter later in this article. Example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int sum=0;
#pragma omp parallel for reduction(+:sum)
for(int n=0; n&lt;1000; ++n) sum += table[n];</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_sections"><a class="anchor" href="#_sections"></a>2.3. Sections</h3>
<div class="paragraph">
<p>Sometimes it is handy to indicate that "this and this can run in parallel". The sections setting is just for that.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp sections
{
    { Work1(); }
    #pragma omp section
    { Work2();
    Work3(); }
    #pragma omp section
    { Work4(); }
}</pre>
</div>
</div>
<div class="paragraph">
<p>This code indicates that any of the tasks Work1, Work2 + Work3 and Work4 may run in parallel, but that Work2 and Work3 must be run in sequence. Each work is done exactly once.</p>
</div>
<div class="paragraph">
<p>As usual, if the compiler ignores the pragmas, the result is still a correctly running program.</p>
</div>
<div class="paragraph">
<p>Internally, GCC implements this as a combination of the parallel for and a switch-case construct. Other compilers may implement it differently.</p>
</div>
<div class="paragraph">
<p>Note: #pragma omp sections only delegates the sections for different threads in the current team. To create a team, you need to specify the parallel keyword either in the surrounding context or in the pragma, as #pragma omp parallel sections.<br>
Example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp parallel sections // starts a new team
{
    { Work1(); }
    #pragma omp section
    { Work2();
    Work3(); }
    #pragma omp section
    { Work4(); }
}</pre>
</div>
</div>
<div class="paragraph">
<p>or</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp parallel // starts a new team
{
//Work0(); // this function would be run by all threads.
    #pragma omp sections // divides the team into sections
    {
        // everything herein is run only once.
        { Work1(); }
        #pragma omp section
        { Work2();
        Work3(); }
        #pragma omp section
        { Work4(); }
    }
//Work5(); // this function would be run by all threads.
}</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_the_simd_construct"><a class="anchor" href="#_the_simd_construct"></a>2.4. The simd construct</h3>
<div class="paragraph">
<p>OpenMP 4.0 added explicit SIMD parallelism (Single-Instruction, Multiple-Data). SIMD means that multiple calculations will be performed simultaneously by the processor, using special instructions that perform the same calculation to multiple values at once. This is often more efficient than regular instructions that operate on single data values. This is also sometimes called <em>vector parallelism</em> or vector operations (and is in fact the preferred term in <em>OpenACC</em>).</p>
</div>
<div class="paragraph">
<p>There are two use cases for the simd construct.</p>
</div>
<div class="paragraph">
<p>Firstly, #pragma omp simd can be used to declare that a loop will be utilizing SIMD.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>float a[8], b[8];
...
#pragma omp simd
for(int n=0; n&lt;8; ++n) a[n] += b[n];</pre>
</div>
</div>
<div class="paragraph">
<p>Secondly, #pragma omp declare simd can be used to indicate a function or procedure that is explicitly designed to take advantage of SIMD parallelism. The compiler may create multiple versions of the same function that use different parameter passing conventions for different CPU capabilities for SIMD processing.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp declare simd aligned(a,b:16)
void add_arrays(float *__restrict__ a, float *__restrict__ b)
{
    #pragma omp simd aligned(a,b:16)
    for(int n=0; n&lt;8; ++n) a[n] += b[n];
}</pre>
</div>
</div>
<div class="paragraph">
<p>Without the pragma, the function will use the default non-SIMD-aware ABI, even though the function itself may do calculation using SIMD.</p>
</div>
<div class="paragraph">
<p>Since compilers of today attempt to do SIMD regardless of OpenMP simd directives, the simd directive can be thought essentially as a directive to the compiler, saying: “Try harder”.</p>
</div>
<div class="sect3">
<h4 id="_the_collapse_clause_2"><a class="anchor" href="#_the_collapse_clause_2"></a>2.4.1. The collapse clause</h4>
<div class="paragraph">
<p>The collapse clause can be added to bind the SIMDness into multiple nested loops. The example code below will direct the compiler into attempting to generate instructions that calculate 16 values simultaneously, if at all possible.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp simd collapse(2)
for(int i=0; i&lt;4; ++i)
    for(int j=0; j&lt;4; ++j)
        a[j*4+i] += b[i*4+j];</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_the_reduction_clause_2"><a class="anchor" href="#_the_reduction_clause_2"></a>2.4.2. The reduction clause</h4>
<div class="paragraph">
<p>The reduction clause can be used with SIMD just like with parallel loops.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int sum=0;
#pragma omp simd reduction(+:sum)
for(int n=0; n&lt;1000; ++n) sum += table[n];</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_the_aligned_clause"><a class="anchor" href="#_the_aligned_clause"></a>2.4.3. The aligned clause</h4>
<div class="paragraph">
<p>The aligned attribute hints the compiler that each element listed is aligned to the given number of bytes. Use this attribute if you are sure that the alignment is guaranteed, and it will increase the performance of the code and make it shorter.</p>
</div>
<div class="paragraph">
<p>The attribute can be used in both the function declaration, and in the individual SIMD statements.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp declare simd aligned(a,b:16)
void add_arrays(float *__restrict__ a, float *__restrict__ b)
{
    #pragma omp simd aligned(a,b:16)
    for(int n=0; n&lt;8; ++n) a[n] += b[n];
}</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_the_safelen_clause"><a class="anchor" href="#_the_safelen_clause"></a>2.4.4. The safelen clause</h4>
<div class="paragraph">
<p>While the restrict keyword in C tells the compiler that it can assume that two pointers will not address the same data (and thus it is safe to change the ordering of reads and writes), the safelen clause in OpenMP provides much fine-grained control over pointer aliasing.</p>
</div>
<div class="paragraph">
<p>In the example code below, the compiler is informed that a[x] and b[y] are independent <em>as long as</em> the difference between x and y is smaller than 4. In reality, the clause controls the upper limit of concurrent loop iterations. It means that only 4 items can be processed concurrently at most. The actual concurrency may be smaller, and depends on the compiler implementation and hardware limits.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp declare simd
void add_arrays(float* a, float* b)
{
    #pragma omp simd aligned(a,b:16) safelen(4)
    for(int n=0; n&lt;8; ++n) a[n] += b[n];
}</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_the_simdlen_clause_openmp_4_5"><a class="anchor" href="#_the_simdlen_clause_openmp_4_5"></a>2.4.5. The simdlen clause (OpenMP 4.5+)</h4>
<div class="paragraph">
<p>The simdlen clause can be added to a declare simd construct to limit how many elements of an array are passed in SIMD registers instead of using the normal parameter passing convention.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_uniform_clause"><a class="anchor" href="#_the_uniform_clause"></a>2.4.6. The uniform clause</h4>
<div class="paragraph">
<p>The uniform clause declares one or more arguments to have an invariant value for all concurrent invocations of the function in the execution of a single SIMD loop.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_linear_clause_openmp_4_5"><a class="anchor" href="#_the_linear_clause_openmp_4_5"></a>2.4.7. The linear clause (OpenMP 4.5+)</h4>
<div class="paragraph">
<p>The linear clause is similar to the firstprivate clause discussed later in this article.</p>
</div>
<div class="paragraph">
<p>Consider this example code:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#include &lt;stdio.h&gt;
int b = 10;
int main()
{
    int array[8];
    #pragma omp simd linear(b:2)
    for(int n=0; n&lt;8; ++n) array[n] = b;
    for(int n=0; n&lt;8; ++n) printf("%d\n", array[n]);
}</pre>
</div>
</div>
<div class="paragraph">
<p>What does this code print? If we ignore the SIMD constructs, we can see it should print the sequence 10,10,10,10,10,10,10,10.</p>
</div>
<div class="paragraph">
<p>But, if we enable the OpenMP SIMD construct, the program should now print 10,12,14,16,18,20,22,24. This is because the linear clause tells the compiler, that the value of b inside each iteration of the loop should be a <em>copy</em> of the original value of b before the SIMD construct, plus the loop iteration number, times the linear scale, which is 2 in this case.</p>
</div>
<div class="paragraph">
<p>In essence, it should be equivalent to the following code:</p>
</div>
<div class="paragraph">
<p>int b_original = b;</p>
</div>
<div class="paragraph">
<p>for(int n=0; n&lt;8; ++n) array[n] = b_original + n*2;</p>
</div>
<div class="paragraph">
<p>However, as of GCC version 6.1.0, the linear clause does not seem to be implemented correctly, at least according to my understanding of the specification, so I cannot do more experimentation.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_inbranch_and_notinbranch_clauses"><a class="anchor" href="#_the_inbranch_and_notinbranch_clauses"></a>2.4.8. The inbranch and notinbranch clauses</h4>
<div class="paragraph">
<p>The inbranch clause specifies that the function will always be called from inside a conditional statement of a SIMD loop. The notinbranch clause specifies that the function will never be called from inside a conditional statement of a SIMD loop.</p>
</div>
<div class="paragraph">
<p>The compiler may use this knowledge to optimize the code.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_the_for_simd_construct_openmp_4_0"><a class="anchor" href="#_the_for_simd_construct_openmp_4_0"></a>2.5. The for simd construct (OpenMP 4.0+)</h3>
<div class="paragraph">
<p>The for and simd constructs can be combined, to divide the execution of a loop into multiple threads, and then execute those loop slices in parallel using SIMD.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>float sum(float* table)
{
    float result=0;
    #pragma omp parallel for simd reduction(+:result)
    for(int n=0; n&lt;1000; ++n) result += table[n];
    return result;
}</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_the_task_construct_openmp_3_0"><a class="anchor" href="#_the_task_construct_openmp_3_0"></a>2.6. The task construct (OpenMP 3.0+)</h3>
<div class="paragraph">
<p>When for and sections are too cumbersome, the task construct can be used.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>struct node { node *left, *right; };
extern void process(node* );
void traverse(node* p)
{
    if (p-&gt;left)
    #pragma omp task // p is firstprivate by default
    traverse(p-&gt;left);
    if (p-&gt;right)
    #pragma omp task // p is firstprivate by default
    traverse(p-&gt;right);
    process(p);
}</pre>
</div>
</div>
<div class="paragraph">
<p>In the next example, we force a postorder traversal of the tree by adding a taskwait directive. Now, we can safely assume that the left and right sons have been executed before we process the current node.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>struct node { node *left, *right; };
extern void process(node* );
void postorder_traverse(node* p)
{
    if (p-&gt;left)
    #pragma omp task // p is firstprivate by default
    postorder_traverse(p-&gt;left);
    if (p-&gt;right)
    #pragma omp task // p is firstprivate by default
    postorder_traverse(p-&gt;right);
    #pragma omp taskwait
    process(p);
}</pre>
</div>
</div>
<div class="paragraph">
<p>The following example demonstrates how to use the task construct to process elements of a linked list in parallel. The pointer p is firstprivate by default on the task construct so it is not necessary to specify it in a firstprivate clause.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>struct node { int data; node* next; };
extern void process(node* );
void increment_list_items(node* head)
{
    #pragma omp parallel
    {
        #pragma omp single
        {
            for(node* p = head; p; p = p-&gt;next)
            {
            #pragma omp task
            process(p); // p is firstprivate by default
            }
        }
    }
}</pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_offloading_support"><a class="anchor" href="#_offloading_support"></a>3. Offloading support</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Offloading means that parts of the program can be executed not only on the CPU of the computer itself, but also in other hardware attached to it, such as on the graphics card.</p>
</div>
<div class="sect2">
<h3 id="_the_declare_target_and_end_declare_target_directives"><a class="anchor" href="#_the_declare_target_and_end_declare_target_directives"></a>3.1. The declare target and end declare target directives</h3>
<div class="paragraph">
<p>The declare target and end declare target directives delimit a section of the source code wherein all declarations, whether they are variables or functions/subroutines, are compiled for a device.</p>
</div>
<div class="paragraph">
<p>Example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp declare target
int x;
void murmur() { x+=5; }
#pragma omp end declare target</pre>
</div>
</div>
<div class="paragraph">
<p>This creates one or more versions of "x" and "murmur". A set that exists on the host computer, and also a separate set that exists and can be run on a device.</p>
</div>
<div class="paragraph">
<p>These two functions and variables are separate, and may contain values separate from each others.</p>
</div>
<div class="paragraph">
<p>Variables declared in this manner can be accessed by the device code without separate map clauses.</p>
</div>
<div class="paragraph">
<p><strong>OpenACC differences</strong></p>
</div>
<div class="paragraph">
<p>In OpenACC, device-functions are declared by prefixing each function with #pragma acc routine. Its data model is more complicated and has no direct translation from/to OpenMP.</p>
</div>
</div>
<div class="sect2">
<h3 id="_the_target_target_data_constructs"><a class="anchor" href="#_the_target_target_data_constructs"></a>3.2. The target, target data constructs</h3>
<div class="paragraph">
<p>The target data construct creates a device data environment.</p>
</div>
<div class="paragraph">
<p>The target construct executes the construct on a device (and also has target data features).</p>
</div>
<div class="paragraph">
<p>These two constructs are identical in effect:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp target
{
    &lt;&lt;statements...&gt;&gt;
}</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>And:</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp target data // device()... map()... if()...
{
    #pragma omp target
    {
        &lt;&lt;statements...&gt;&gt;
    }
}</pre>
</div>
</div>
<div class="paragraph">
<p><strong>IMPORTANT</strong>: The target construct does not add any parallelism to the program by itself. It only transfers the execution into another device, and executes the code there in a single thread.</p>
</div>
<div class="paragraph">
<p>To utilize parallelism on device, you have to engage a teams construct inside the target construct. Example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#include &lt;stdio.h&gt;
long long r = 1;
int main(void)
{
    r=10;
    #pragma omp target teams distribute parallel for reduction(+:r) map(tofrom:r)
    for(unsigned long long n=0; n&lt;0x800000000ull; ++n) r += n;
    printf( "r=%llX\n", r );
    return 0;
}</pre>
</div>
</div>
<div class="paragraph">
<p>See the teams keyword below for details.</p>
</div>
<div class="sect3">
<h4 id="_the_if_clause"><a class="anchor" href="#_the_if_clause"></a>3.2.1. The if clause</h4>
<div class="paragraph">
<p>If an if clause is added to the target region, the attached expression is evaluated. If the expression returns false, the code is only executed on the host. Otherwise, or if the if clause is not used, the code is executed on the device, and the task will wait until the device is done with the processing.</p>
</div>
<div class="paragraph">
<p>Example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;
int main(int argc, char** argv)
{
    int r=0;
    #pragma omp target if(atoi(argv[1])) map(tofrom:r)
    r += 4;
    printf("r=%d\n", r);
}</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_the_device_clause"><a class="anchor" href="#_the_device_clause"></a>3.2.2. The device clause</h4>
<div class="paragraph">
<p>Specifices the particular device that is to execute the code.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int device_number = ...;
#pragma omp target device(device_number)
{
    //...
}</pre>
</div>
</div>
<div class="paragraph">
<p>You can acquire device numbers by using the &lt;omp.h&gt; library functions, such as omp_set_default_device, omp_get_default_device, omp_get_num_devices, and omp_is_initial_device.</p>
</div>
<div class="paragraph">
<p>If the device clause is not used, the code is executed on the default device. The default device number is controlled by the omp_set_default_device function, or the OMP_DEFAULT_DEVICE environment variable.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_map_clause"><a class="anchor" href="#_the_map_clause"></a>3.2.3. The map clause</h4>
<div class="paragraph">
<p>The map clause controls how data is between the host and the device.</p>
</div>
<div class="paragraph">
<p>There are four different types of mappings:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>map(alloc:variables) specifies that at entry to the block, the specified variables have uninitialized values.</p>
</li>
<li>
<p>map(from:variables) specifies that at entry to the block, the specified variables have copies of their original values on the host.</p>
</li>
<li>
<p>map(to:variables) specifies that at exit from the block, the values of these variables will be copied back to the host.</p>
</li>
<li>
<p>map(tofrom:variables) is a combination of from and to. This is the default mapping.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Variables are initialized and assigned through bitwise copy, i.e. constructors / operators are not called.</p>
</div>
<div class="paragraph">
<p>The mapping items can be entire variables or array sections.</p>
</div>
</div>
<div class="sect3">
<h4 id="_array_sections_openmp_4_0"><a class="anchor" href="#_array_sections_openmp_4_0"></a>3.2.4. Array sections (OpenMP 4.0+)</h4>
<div class="paragraph">
<p>The variables in map and depend can also specify array sections. The array subsections are defined using one of the following syntax:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>[lowerbound:length])</p>
</li>
<li>
<p>[lowerbound:])</p>
</li>
<li>
<p>[:length])</p>
</li>
<li>
<p>[:])</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Array sections can only be specified in the map, and depend clauses. They are invalid in e.g private.</p>
</div>
<div class="paragraph">
<p>An example of a valid array subscript mapping:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>void foo (int *p)
{
    int i;
    #pragma omp parallel
    #pragma omp single
    #pragma omp target teams distribute parallel for map(p[0:24])
    for (i = 0; i &lt; 24; i++) p[i] = p[i] + 1;
}</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_the_target_enter_data_and_target_exit_data_constructs_openmp_4_5"><a class="anchor" href="#_the_target_enter_data_and_target_exit_data_constructs_openmp_4_5"></a>3.3. The target enter data and target exit data constructs (OpenMP 4.5+)</h3>
<div class="paragraph">
<p>While the map clauses within a target data construct can be used to allocate data in the device memory and automatically deallocate it in the end of the construct, the target enter data and target exit data constructs can be used to store data in the memory in a more persistent manner.</p>
</div>
<div class="paragraph">
<p>Examples:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>#pragma omp target enter data map(from:var)</p>
</li>
<li>
<p>#pragma omp target exit data map(to:var)</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_the_target_update_construct"><a class="anchor" href="#_the_target_update_construct"></a>3.4. The target update construct</h3>
<div class="paragraph">
<p>The target update construct can be used to synchronize data between the device memory and the host memory without deallocating it.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>#pragma omp target update from(c)</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_teams"><a class="anchor" href="#_teams"></a>4. Teams</h2>
<div class="sectionbody">
<div class="paragraph">
<p>While the parallel construct creates a <em>team</em> of <em>threads</em>, the teams construct creates a <em>league</em> of <em>teams</em>.</p>
</div>
<div class="paragraph">
<p>This directive can be only used directly inside a target construct. The optional attribute num_teams can be used to specify the maximum number of teams created. The actual number of teams may be smaller than this number. The <em>master</em> thread of each team will execute the code inside that team.</p>
</div>
<div class="paragraph">
<p>The example code below <em>may</em> print the message multiple times.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#include &lt;stdio.h&gt;
int main(void)
{
    #pragma omp target teams
    {
    printf("test\n");
    }
    return 0;
}</pre>
</div>
</div>
<div class="paragraph">
<p><strong>OpenACC differences</strong></p>
</div>
<div class="paragraph">
<p>OpenACC calls teams and threads <em>gangs</em> and <em>workers</em> respectively. In OpenACC, a set of new teams is launched on the device with #pragma acc parallel, with the optional attribute num_gangs(n). This combines the behavior of #pragma omp target and #pragma omp teams.</p>
</div>
<div class="sect2">
<h3 id="_the_distribute_construct"><a class="anchor" href="#_the_distribute_construct"></a>4.1. The distribute construct</h3>
<div class="paragraph">
<p>The distribute construct can be used to distribute a for loop across the <em>master</em> threads of all teams of the current teams region.</p>
</div>
<div class="paragraph">
<p>For example, if there are 20 teams, the loop will be distributed across 20 <em>master</em> threads.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#include &lt;stdio.h&gt;
int main(void)
{
    int r=0;
    #pragma omp target teams distribute reduction(+:r)
    for(int n=0; n&lt;10000; ++n)
    r += n;
    printf("r=%d\n", r);
    return 0;
}</pre>
</div>
</div>
<div class="paragraph">
<p><strong>OpenACC differences</strong></p>
</div>
<div class="paragraph">
<p>In OpenACC this behavior is achieved by adding the word gang to existing worksharing constructs like #pragma acc parallel and #pragma acc kernels.</p>
</div>
<div class="sect3">
<h4 id="_the_distribute_simd_construct"><a class="anchor" href="#_the_distribute_simd_construct"></a>4.1.1. The distribute simd construct</h4>
<div class="paragraph">
<p>Adding the simd clause into the distribute construct will combine the effects of simd and distribute, meaning that the loop will be divided across the <em>master</em> threads of all teams of the current teams region, and therein divided according to the same principles that are in effect in #pragma omp simd constructs.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_dist_schedule_clause"><a class="anchor" href="#_the_dist_schedule_clause"></a>4.1.2. The dist_schedule clause</h4>
<div class="paragraph">
<p>Much like with the schedule clause used with for scheduling, the scheduling in distribute can be controlled with the dist_schedule clause. Currently the only possible value for dist_schedule is static.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_the_distribute_parallel_for_construct"><a class="anchor" href="#_the_distribute_parallel_for_construct"></a>4.2. The distribute parallel for construct</h3>
<div class="paragraph">
<p>The distribute parallel for construct can be used to distribute a for loop across <em>all</em> threads of all teams of the current teams region.</p>
</div>
<div class="paragraph">
<p>For example, if there are 20 teams, and each team consists of 256 threads, the loop will be distributed across 5120 threads.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#include &lt;stdio.h&gt;
int main(void)
{
    int r=0;
    #pragma omp target teams distribute parallel for reduction(+:r)
    for(int n=0; n&lt;10000; ++n)
    r += n;
    printf("r=%d\n", r);
    return 0;
}</pre>
</div>
</div>
<div class="paragraph">
<p>The number of threads created in each team is implementation defined, but can be explicitly defined with the num_threads attribute.</p>
</div>
<div class="paragraph">
<p>The simd clause can be added once again to the loop to add SIMD execution, if possible.</p>
</div>
<div class="paragraph">
<p><strong>OpenACC differences</strong></p>
</div>
<div class="paragraph">
<p>In OpenACC this behavior is achieved by adding the word worker to existing worksharing constructs like #pragma acc parallel and #pragma acc kernels. Additionally the word vector can be added to achieve SIMD parallelism as well.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_thread_safety_i_e_mutual_exclusion"><a class="anchor" href="#_thread_safety_i_e_mutual_exclusion"></a>5. Thread-safety (i.e. mutual exclusion)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>There are a wide array of concurrency and mutual exclusion problems related to multithreading programs. I won&#8217;t explain them here in detail; there are many good books dealing with the issue. (For example, <em>Multithreaded, Parallel, and Distributed Programming</em> by Gregory R. Andrews.)</p>
</div>
<div class="paragraph">
<p>Instead, I will explain the tools that OpenMP provides to handle mutual exclusion correctly.</p>
</div>
<div class="sect2">
<h3 id="_atomicity"><a class="anchor" href="#_atomicity"></a>5.1. Atomicity</h3>
<div class="paragraph">
<p>Atomicity means that something is inseparable; an event either happens completely or it does not happen at all, and another thread cannot intervene during the execution of the event.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp atomic
counter += value;</pre>
</div>
</div>
<div class="paragraph">
<p>The atomic keyword in OpenMP specifies that the denoted action happens atomically. It is commonly used to update counters and other simple variables that are accessed by multiple threads simultaneously.</p>
</div>
<div class="paragraph">
<p>See also reduction.</p>
</div>
<div class="paragraph">
<p>There are four different types of atomic expressions (since OpenMP 3.1):</p>
</div>
<div class="sect3">
<h4 id="_atomic_read_expressions"><a class="anchor" href="#_atomic_read_expressions"></a>5.1.1. Atomic read expressions</h4>
<div class="literalblock">
<div class="content">
<pre>#pragma omp atomic read
var = x;</pre>
</div>
</div>
<div class="paragraph">
<p>Here the reading of x is guaranteed to happen atomically, but nothing is guaranteed about var. Note that var may not access the memory location designated for x.</p>
</div>
</div>
<div class="sect3">
<h4 id="_atomic_write_expressions"><a class="anchor" href="#_atomic_write_expressions"></a>5.1.2. Atomic write expressions</h4>
<div class="literalblock">
<div class="content">
<pre>#pragma omp atomic write
x = expr;</pre>
</div>
</div>
<div class="paragraph">
<p>Here the writing of x is guaranteed to happen atomically, but nothing is guaranteed about expr. Note that expr may not access the memory location designated for x.</p>
</div>
</div>
<div class="sect3">
<h4 id="_atomic_update_expressions"><a class="anchor" href="#_atomic_update_expressions"></a>5.1.3. Atomic update expressions</h4>
<div class="literalblock">
<div class="content">
<pre>#pragma omp atomic update // The word "update" is optional
// One of these:
++x; --x; x++; x--;
x += expr; x -= expr; x *= expr; x /= expr; x &amp;= expr;
x = x+expr; x = x-expr; x = x*expr; x = x/expr; x = x&amp;expr;
x = expr+x; x = expr-x; x = expr*x; x = expr/x; x = expr&amp;x;
x |= expr; x ^= expr; x &lt;&lt;= expr; x &gt;&gt;= expr;
x = x|expr; x = x^expr; x = x&lt;&lt;expr; x = x&gt;&gt;expr;
x = expr|x; x = expr^x; x = expr&lt;&lt;x; x = expr&gt;&gt;x;</pre>
</div>
</div>
<div class="paragraph">
<p>Here the updating of x is guaranteed to happen atomically, but nothing is guaranteed about expr. Note that expr may not access the memory location designated for x.</p>
</div>
</div>
<div class="sect3">
<h4 id="_atomic_capture_expressions"><a class="anchor" href="#_atomic_capture_expressions"></a>5.1.4. Atomic capture expressions</h4>
<div class="paragraph">
<p>Capture expressions combine the read and update features.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp atomic capture
// One of these:
var = x++; /* Or any other of the update expressions listed above */
{ var = x; x++; /* Or any other of of the update expressions listed above */ }
{ x++; /* Or any other of of the update expressions listed above */; var = x; }
{ var = x; x = expr; }</pre>
</div>
</div>
<div class="paragraph">
<p>Note that neither var nor expr may not access the memory location designated for x.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_the_critical_construct"><a class="anchor" href="#_the_critical_construct"></a>5.2. The critical construct</h3>
<div class="paragraph">
<p>The critical construct restricts the execution of the associated statement / block to a single thread at time.</p>
</div>
<div class="paragraph">
<p>The critical construct may optionally contain a global name that identifies the type of the critical construct. No two threads can execute a critical construct of the same name at the same time.</p>
</div>
<div class="paragraph">
<p>If the name is omitted, a default name is assumed.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp critical(dataupdate)
{
    datastructure.reorganize();
}
...
#pragma omp critical(dataupdate)
{
    datastructure.reorganize_again();
}</pre>
</div>
</div>
<div class="paragraph">
<p>In this example, only one of the critical sections named "dataupdate" may be executed at any given time, and only one thread may be executing it at that time. I.e. the functions "reorganize" and "reorganize_again" cannot be invoked at the same time, and two calls to the function cannot be active at the same time. (Except if other calls exist elsewhere, unprotected by the critical construct.)</p>
</div>
<div class="paragraph">
<p>Note: The critical section names are global to the entire program (regardless of module boundaries). So if you have a critical section by the same name in multiple modules, not two of them can be executed at the same time.</p>
</div>
<div class="paragraph">
<p>If you need something like a local mutex, see below.</p>
</div>
</div>
<div class="sect2">
<h3 id="_locks"><a class="anchor" href="#_locks"></a>5.3. Locks</h3>
<div class="paragraph">
<p>The OpenMP runtime library provides a lock type, omp_lock_t in its include file, omp.h.</p>
</div>
<div class="paragraph">
<p>The lock type has five manipulator functions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>omp_init_lock initializes the lock. After the call, the lock is unset.</p>
</li>
<li>
<p>omp_destroy_lock destroys the lock. The lock must be unset before this call.</p>
</li>
<li>
<p>omp_set_lock attempts to set the lock. If the lock is already set by another thread, it will wait until the lock is no longer set, and then sets it.</p>
</li>
<li>
<p>omp_unset_lock unsets the lock. It should only be called by the same thread that set the lock; the consequences of doing otherwise are undefined.</p>
</li>
<li>
<p>omp_test_lock attempts to set the lock. If the lock is already set by another thread, it returns 0; if it managed to set the lock, it returns 1.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Here is an example of a wrapper around std::set&lt;&gt; that provides per-instance mutual exclusion while still working even if the compiler does not support OpenMP.</p>
</div>
<div class="paragraph">
<p>You can maintain backward compability with non-OpenMP-supporting compilers by enclosing the library references in #ifdef _OPENMP…#endif blocks.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#ifdef _OPENMP
# include &lt;omp.h&gt;
#endif
#include &lt;set&gt;
class data
{
    private:
        std::set&lt;int&gt; flags;
        #ifdef _OPENMP
        omp_lock_t lock;
        #endif
    public:
    data() : flags()
    {
        #ifdef _OPENMP
        omp_init_lock(&amp;lock);
        #endif
    }
    ~data()
    {
        #ifdef _OPENMP
        omp_destroy_lock(&amp;lock);
        #endif
    }
    bool set_get(int c)
    {
        #ifdef _OPENMP
        omp_set_lock(&amp;lock);
        #endif
        bool found = flags.find(c) != flags.end();
        if(!found) flags.insert(c);
        #ifdef _OPENMP
        omp_unset_lock(&amp;lock);
        #endif
        return found;
    }
};</pre>
</div>
</div>
<div class="paragraph">
<p>Of course, you would really rather wrap the lock into a custom container to avoid littering the code with #ifdefs and also for providing exception-safety:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>    #ifdef _OPENMP
    # include &lt;omp.h&gt;
    struct MutexType
    {
        MutexType() { omp_init_lock(&amp;lock); }
        ~MutexType() { omp_destroy_lock(&amp;lock); }
        void Lock() { omp_set_lock(&amp;lock); }
        void Unlock() { omp_unset_lock(&amp;lock); }
        MutexType(const MutexType&amp; ) { omp_init_lock(&amp;lock); }
        MutexType&amp; operator= (const MutexType&amp; ) { return *this; }
        public:
        omp_lock_t lock;
    };
#else
    /* A dummy mutex that doesn't actually exclude anything,
    * but as there is no parallelism either, no worries. */
    struct MutexType
    {
        void Lock() {}
        void Unlock() {}
    };
    #endif
    /* An exception-safe scoped lock-keeper. */
    struct ScopedLock
    {
        explicit ScopedLock(MutexType&amp; m) : mut(m), locked(true) { mut.Lock(); }
        ~ScopedLock() { Unlock(); }
        void Unlock() { if(!locked) return; locked=false; mut.Unlock(); }
        void LockAgain() { if(locked) return; mut.Lock(); locked=true; }
        private:
        MutexType&amp; mut;
        bool locked;
        private: // prevent copying the scoped lock.
        void operator=(const ScopedLock&amp;);
        ScopedLock(const ScopedLock&amp;);
    };</pre>
</div>
</div>
<div class="paragraph">
<p>This way, the example above becomes a lot simpler, and also exception-safe:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#include &lt;set&gt;
class data
{
    private:
        std::set&lt;int&gt; flags;
        MutexType lock;
        public:
        bool set_get(int c)
        {
            ScopedLock lck(lock); // locks the mutex
            if(flags.find(c) != flags.end()) return true; // was found
            flags.insert(c);
            return false; // was not found
        } // automatically releases the lock when lck goes out of scope.
};</pre>
</div>
</div>
<div class="paragraph">
<p>There is also a lock type that supports nesting, omp_nest_lock_t. I will not cover it here.</p>
</div>
</div>
<div class="sect2">
<h3 id="_the_flush_directive"><a class="anchor" href="#_the_flush_directive"></a>5.4. The flush directive</h3>
<div class="paragraph">
<p>Even when variables used by threads are supposed to be shared, the compiler may take liberties and optimize them as register variables. This can skew concurrent observations of the variable. The flush directive can be used to ensure that the value observed in one thread is also the value observed by other threads.</p>
</div>
<div class="paragraph">
<p>This example comes from the OpenMP specification.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>/* presumption: int a = 0, b = 0; */
/* First thread */ /* Second thread */
b = 1; a = 1;
#pragma omp flush(a,b) #pragma omp flush(a,b)
if(a == 0) if(b == 0)
{ {
    /* Critical section */ /* Critical section */
} }</pre>
</div>
</div>
<div class="paragraph">
<p>In this example, it is enforced that at the time either of a or b is accessed, the other is also up-to-date, practically ensuring that not both of the two threads enter the critical section. (Note: It is still possible that neither of them can enter it.)</p>
</div>
<div class="paragraph">
<p>You need the flush directive when you have writes to and reads from the same data in different threads.</p>
</div>
<div class="paragraph">
<p><strong>If the program appears to work correctly without the flush directive, it does not mean that the flush directive is not required.</strong> It just may be that your compiler is not utilizing all the freedoms the standard allows it to do. You <em>need</em> the flush directive whenever you access shared data in multiple threads: After a write, before a read.</p>
</div>
<div class="paragraph">
<p>However, I do not know these:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Is flush needed if the shared variable is declared volatile?</p>
</li>
<li>
<p>Is flush needed if all access to the shared variable is atomic or restricted by critical sections?</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_controlling_which_data_to_share_between_threads"><a class="anchor" href="#_controlling_which_data_to_share_between_threads"></a>6. Controlling which data to share between threads</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In the parallel section, it is possible to specify which variables are shared between the different threads and which are not. By default, all variables are shared except those declared within the parallel block.</p>
</div>
<div class="sect2">
<h3 id="_the_private_firstprivate_and_shared_clauses"><a class="anchor" href="#_the_private_firstprivate_and_shared_clauses"></a>6.1. The private, firstprivate and shared clauses</h3>
<div class="literalblock">
<div class="content">
<pre>int a, b=0;
#pragma omp parallel for private(a) shared(b)
for(a=0; a&lt;50; ++a)
{
    #pragma omp atomic
    b += a;
}</pre>
</div>
</div>
<div class="paragraph">
<p>This example explicitly specifies that a is private (each thread has their own copy of it) and that b is shared (each thread accesses the same variable).</p>
</div>
<div class="sect3">
<h4 id="_the_difference_between_private_and_firstprivate"><a class="anchor" href="#_the_difference_between_private_and_firstprivate"></a>6.1.1. The difference between private and firstprivate</h4>
<div class="paragraph">
<p>Note that a private copy is an uninitialized variable by the same name and same type as the original variable; it does <em>not</em> copy the value of the variable that was in the surrounding context.</p>
</div>
<div class="paragraph">
<p>Example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#include &lt;string&gt;
#include &lt;iostream&gt;
int main()
{
    std::string a = "x", b = "y";
    int c = 3;
#pragma omp parallel private(a,c) shared(b) num_threads(2)
{
    a += "k";
    c += 7;
    std::cout &lt;&lt; "A becomes (" &lt;&lt; a &lt;&lt; "), b is (" &lt;&lt; b &lt;&lt; ")\n";
}
}</pre>
</div>
</div>
<div class="paragraph">
<p>This will output the string "k", not "xk". At the entrance of the block, a becomes a new instance of std::string, that is initialized with the default constructor; it is not initialized with the copy constructor.</p>
</div>
<div class="paragraph">
<p>Internally, the program becomes like this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int main()
{
    std::string a = "x", b = "y";
    int c = 3;
    OpenMP_thread_fork(2);
    { // Start new scope
        std::string a; // Note: It is a new local variable.
        int c; // This too.
        a += "k";
        c += 7;
        std::cout &lt;&lt; "A becomes (" &lt;&lt; a &lt;&lt; "), b is (" &lt;&lt; b &lt;&lt; ")\n";
    } // End of scope for the local variables
    OpenMP_join();
}</pre>
</div>
</div>
<div class="paragraph">
<p>In the case of primitive (POD) datatypes (int, float, char* etc.), the private variable is uninitialized, just like any declared but not initialized local variable. It does not contain the value of the variable from the surrounding context. Therefore, the increment of c is moot here; the value of the variable is still undefined. (If you are using GCC version earlier than 4.4, you do not even get a warning about the use of uninitialized value in situations like this.)</p>
</div>
<div class="paragraph">
<p>If you actually need a <em>copy</em> of the original value, use the firstprivate clause instead.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#include &lt;string&gt;
#include &lt;iostream&gt;
int main()
{
    std::string a = "x", b = "y";
    int c = 3;
        #pragma omp parallel firstprivate(a,c) shared(b) num_threads(2)
        {
        a += "k";
        c += 7;
        std::cout &lt;&lt; "A becomes (" &lt;&lt; a &lt;&lt; "), b is (" &lt;&lt; b &lt;&lt; ")\n";
        }
}</pre>
</div>
</div>
<div class="paragraph">
<p>Now the output becomes "A becomes (xk), b is (y)".</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_the_lastprivate_clause"><a class="anchor" href="#_the_lastprivate_clause"></a>6.2. The lastprivate clause</h3>
<div class="paragraph">
<p>The lastprivate clause defines a variable private as in firstprivate or private, but causes the value from the last task to be copied back to the original value after the end of the loop/sections construct.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>In a loop construct (for construct), the last value is the value assigned by the thread that handles the last iteration of the loop. Values assigned during other iterations are ignored.</p>
</li>
<li>
<p>In a sections construct (sections construct), the last value is the value assigned in the last section denoted by the section construct. Values assigned in other sections are ignored.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#include &lt;stdio.h&gt;
int main()
{
    int done = 4, done2 = 5;
    #pragma omp parallel for lastprivate(done, done2) num_threads(2) schedule(static)
    for(int a=0; a&lt;8; ++a)
    {
        if(a==2) done=done2=0;
        if(a==3) done=done2=1;
    }
    printf("%d,%d\n", done,done2);
}</pre>
</div>
</div>
<div class="paragraph">
<p>This program outputs "4196224,-348582208", because internally, this program became like this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#include &lt;stdio.h&gt;
int main()
{
int done = 4, done2 = 5;
OpenMP_thread_fork(2);
    {
        int this_thread = omp_get_thread_num(), num_threads = 2;
        int my_start = (this_thread ) * 8 / num_threads;
        int my_end = (this_thread+1) * 8 / num_threads;
        int priv_done, priv_done2; // not initialized, because firstprivate was not used
        for(int a=my_start; a&lt;my_end; ++a)
        {
            if(a==2) priv_done=priv_done2=0;
            if(a==3) priv_done=priv_done2=1;
        }
        if(my_end == 8)
        {
            // assign the values back, because this was the last iteration
            done = priv_done;
            done2 = priv_done2;
        }
    }
OpenMP_join();
}</pre>
</div>
</div>
<div class="paragraph">
<p>As one can observe, the values of priv_done and priv_done2 are not assigned even once during the course of the loop that iterates through 4&#8230;&#8203;7. As such, the values that are assigned back are completely bogus.</p>
</div>
<div class="paragraph">
<p>Therefore, lastprivate cannot be used to e.g. fetch the value of a flag assigned randomly during a loop. Use reduction for that, instead.</p>
</div>
<div class="paragraph">
<p>Where this behavior <em>can</em> be utilized though, is in situations like this (from OpenMP manual):</p>
</div>
<div class="literalblock">
<div class="content">
<pre>void loop()
{
    int i;
    #pragma omp for lastprivate(i)
    for(i=0; i&lt;get_loop_count(); ++i) // note: get_loop_count() must be a pure function.
    { ... }
    printf("%d\n", i); // this shows the number of loop iterations done.
    }</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_the_default_clause"><a class="anchor" href="#_the_default_clause"></a>6.3. The default clause</h3>
<div class="paragraph">
<p>The most useful purpose on the default clause is to check whether you have remembered to consider all variables for the private/shared question, using the default(none) setting.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int a, b=0;
#pragma omp parallel default(none) shared(b)
{
    b += a;
}</pre>
</div>
</div>
<div class="paragraph">
<p>The default clause can also be used to set that all variables are shared by default (default(shared)).</p>
</div>
<div class="paragraph">
<p>Note: Because different compilers have different ideas about which variables are <em>implicitly</em> private or shared, and for which it is an <em>error</em> to explicitly state the private/shared status, it is recommended to use the default(none) setting only during development, and drop it in production/distribution code.</p>
</div>
</div>
<div class="sect2">
<h3 id="_the_reduction_clause_3"><a class="anchor" href="#_the_reduction_clause_3"></a>6.4. The reduction clause</h3>
<div class="paragraph">
<p>The reduction clause is a mix between the private, shared, and atomic clauses.<br>
It allows to accumulate a shared variable without the atomic clause, but the type of accumulation must be specified. It will often produce faster executing code than by using the atomic clause.</p>
</div>
<div class="paragraph">
<p>This example calculates using threads:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int factorial(int number)
{
    int fac = 1;
    #pragma omp parallel for reduction(*:fac)
    for(int n=2; n&lt;=number; ++n)
    fac *= n;
    return fac;
}</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>At the beginning of the parallel block, a private copy is made of the variable and preinitialized to a certain value .</p>
</li>
<li>
<p>At the end of the parallel block, the private copy is atomically merged into the shared variable using the defined operator.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>(The private copy is actually just a new local variable by the same name and type; the original variable is not accessed to create the copy.)</p>
</div>
<div class="paragraph">
<p>The syntax of the clause is:</p>
</div>
<div class="paragraph">
<p>reduction(<em>operator</em>:_list_)</p>
</div>
<div class="paragraph">
<p>where <em>list</em> is the list of variables where the operator will be applied to, and <em>operator</em> is one of these:</p>
</div>
<div class="paragraph">
<p>|==
|Operator |Initialization value
|+, -, \|, ^, \|\| |0
|*, &amp;&amp; |1
|&amp; |~0
|min |largest representable number
|max |smallest representable number
|==</p>
</div>
<div class="paragraph">
<p>To write the factorial function (shown above) without reduction, it probably would look like this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int factorial(int number)
{
    int fac = 1;
    #pragma omp parallel for
    for(int n=2; n&lt;=number; ++n)
    {
        #pragma omp atomic
        fac *= n;
    }
    return fac;
}</pre>
</div>
</div>
<div class="paragraph">
<p>However, this code would be less optimal than the one with reduction: it misses the opportunity to use a local (possible register) variable for the cumulation, and needlessly places load/synchronization demands on the shared memory variable. In fact, due to the bottleneck of that atomic variable (only one thread may access it simultaneously), it would completely nullify any gains of parallelism in that loop.</p>
</div>
<div class="paragraph">
<p>The version with reduction is equivalent to this code (illustration only):</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int factorial(int number)
{
    int fac = 1;
    #pragma omp parallel
    {
        int omp_priv = 1; /* This value comes from the table shown above */
        #pragma omp for nowait
        for(int n=2; n&lt;=number; ++n)
        omp_priv *= n;
        #pragma omp atomic
        fac *= omp_priv;
    }
return fac;
}</pre>
</div>
</div>
<div class="paragraph">
<p>Note how it moves the atomic operation out from the loop.</p>
</div>
<div class="paragraph">
<p>The restrictions in reduction and atomic are very similar: both can only be done on POD types; neither allows overloaded operators, and both have the same set of supported operators.</p>
</div>
<div class="paragraph">
<p>As an example of how the reduction clause can be used to produce semantically different code when OpenMP is enabled and when it is disabled, this example prints the number of threads that executed the parallel block:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>int a = 0;
#pragma omp parallel reduction (+:a)
{
    a = 1; // Assigns a value to the private copy.
    // Each thread increments the value by 1.
}</pre>
</div>
</div>
<div class="paragraph">
<p>printf("%d\n", a);</p>
</div>
<div class="paragraph">
<p>If you preinitialized "a" to 4, it would print a number &gt;= 5 if OpenMP was enabled, and 1 if OpenMP was disabled.<br>
<em>Note: If you really need to detect whether OpenMP is enabled, use the</em> <em>OPENMP _#define instead. To get the number of threads, use</em> omp_get_num_threads() <em>instead.</em></p>
</div>
<div class="sect3">
<h4 id="_the_declare_reduction_directive_openmp_4_0"><a class="anchor" href="#_the_declare_reduction_directive_openmp_4_0"></a>6.4.1. The declare reduction directive (OpenMP 4.0+)</h4>
<div class="paragraph">
<p>The declare reduction directive generalizes the reductions to include user-defined reductions.</p>
</div>
<div class="paragraph">
<p>The syntax of the declaration is one of these two:</p>
</div>
<div class="paragraph">
<p>#pragma omp declare reduction(name:type:expression)<br>
#pragma omp declare reduction(name:type:expression) initializer(expression)</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <em>name</em> is the name you want to give to the reduction method.</p>
</li>
<li>
<p>The <em>type</em> is the type of your reduction result.</p>
</li>
<li>
<p>Within the reduction expression, the special variables omp_in and omp_out are implicitly declared, and they stand for the input and output expressions respectively.</p>
</li>
<li>
<p>Within the initializer expression, the special variable omp_priv is implicitly declared and stands for the initial value of the reduction result.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>An example use case is when you are running a data compressor with different parameters, and you want to find the set of parameters that results in best compression. Below is an example of such code:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#include &lt;cstdio&gt;
int compress(int param1, int param2)
{
return (param1+13)^param2; // Placeholder for a compression algorithm
}
int main(int argc, char** argv)
{
    struct BestInfo { unsigned size, param1, param2; };
    #pragma omp declare reduction(isbetter:BestInfo: \
    omp_in.size&lt;omp_out.size ? omp_out=omp_in : omp_out \
    ) initializer(omp_priv = BestInfo{~0u,~0u,~0u})
    BestInfo result{~0u,~0u,~0u};
    #pragma omp parallel for collapse(2) reduction(isbetter:result)
    for(unsigned p1=0; p1&lt;10; ++p1)
        for(unsigned p2=0; p2&lt;10; ++p2)
        {
        unsigned size = compress(p1,p2);
        if(size &lt; result.size) result = BestInfo{size,p1,p2};
        }
    std::printf("Best compression (%u bytes) with params %u,%u\n",
    result.size, result.param1, result.param2);
}</pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_thread_affinity_proc_bind"><a class="anchor" href="#_thread_affinity_proc_bind"></a>7. Thread affinity (proc_bind)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The thread affinity of the parallel construct can be controlled with a proc_bind clause. It takes one of the following three forms:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>#pragma omp parallel proc_bind(master)</p>
</li>
<li>
<p>#pragma omp parallel proc_bind(close)</p>
</li>
<li>
<p>#pragma omp parallel proc_bind(spread)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For more information, read the OpenMP specification.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_execution_synchronization"><a class="anchor" href="#_execution_synchronization"></a>8. Execution synchronization</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_the_barrier_directive_and_the_nowait_clause"><a class="anchor" href="#_the_barrier_directive_and_the_nowait_clause"></a>8.1. The barrier directive and the nowait clause</h3>
<div class="paragraph">
<p>The barrier directive causes threads encountering the barrier to wait until all the other threads in the same team have encountered the barrier.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp parallel
{
    /* All threads execute this. */
    SomeCode();
    #pragma omp barrier
    /* All threads execute this, but not before
    * all threads have finished executing SomeCode().
    */
    SomeMoreCode();
}</pre>
</div>
</div>
<div class="paragraph">
<p>Note: There is an implicit barrier at the end of each parallel block, and at the end of each sections, for and single statement, unless the nowait directive is used.</p>
</div>
<div class="paragraph">
<p>Example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp parallel
{
    #pragma omp for
    for(int n=0; n&lt;10; ++n) Work();
    // This line is not reached before the for-loop is completely finished
    SomeMoreCode();
}
// This line is reached only after all threads from
// the previous parallel block are finished.
CodeContinues();
#pragma omp parallel
{
    #pragma omp for nowait
    for(int n=0; n&lt;10; ++n) Work();
    // This line may be reached while some threads are still executing the for-loop.
    SomeMoreCode();
}
// This line is reached only after all threads from
// the previous parallel block are finished.
CodeContinues();</pre>
</div>
</div>
<div class="paragraph">
<p>The nowait directive can only be attached to sections, for and single. It cannot be attached to the within-loop ordered clause, for example.</p>
</div>
</div>
<div class="sect2">
<h3 id="_the_single_and_master_constructs"><a class="anchor" href="#_the_single_and_master_constructs"></a>8.2. The single and master constructs</h3>
<div class="paragraph">
<p>The single construct specifies that the given statement/block is executed by only one thread. It is unspecified which thread. Other threads skip the statement/block and wait at an implicit barrier at the end of the construct.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp parallel
{
    Work1();
    #pragma omp single
    {
        Work2();
    }
    Work3();
}</pre>
</div>
</div>
<div class="paragraph">
<p>In a 2-cpu system, this will run Work1() twice, Work2() once and Work3() twice. There is an implied barrier at the end of the single construct, but not at the beginning of it.</p>
</div>
<div class="paragraph">
<p>Note: Do not assume that the single block is executed by whichever thread gets there first. According to the standard, the decision of which thread executes the block is implementation-defined, and therefore making assumptions on it is non-conforming.</p>
</div>
<div class="paragraph">
<p>The master construct is similar, except that the statement/block is run by the <em>master</em> thread, and there is no implied barrier; other threads skip the construct without waiting.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp parallel
{
    Work1();
    // This...
    #pragma omp master
    {
        Work2();
    }</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>    // ...is practically identical to this:
    if(omp_get_thread_num() == 0)
    {
        Work2();
    }
    Work3();
}</pre>
</div>
</div>
<div class="paragraph">
<p>Unless you use the threadprivate clause, the only important difference between single nowait and master is that if you have multiple master blocks in a parallel section, you are guaranteed that they are executed by the same thread every time, and hence, the values of private (thread-local) variables are the same.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_thread_cancellation_openmp_4_0"><a class="anchor" href="#_thread_cancellation_openmp_4_0"></a>9. Thread cancellation (OpenMP 4.0+)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Suppose that we want to optimize this function with parallel processing:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>/* Returns any position from the haystack where the needle can
* be found, or NULL if no such position exists. It is not guaranteed
* to find the first matching position; it only guarantees to find
* _a_ matching position if one exists.
*/</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>const char* FindAnyNeedle(const char* haystack, size_t size, char needle)
{
    for(size_t p = 0; p &lt; size; ++p)
        if(haystack[p] == needle)
        {
            /* This breaks out of the loop. */
            return haystack+p;
        }
    return NULL;
}</pre>
</div>
</div>
<div class="paragraph">
<p>Our first attempt might be to simply tack a #pragma parallel for before the for loop, but that doesn&#8217;t work: OpenMP requires that a loop construct processes each iteration. Breaking out of the loop (using return, goto, break, throw or other means) is not allowed.</p>
</div>
<div class="paragraph">
<p>To solve this problem, OpenMP 4.0 added a mechanism called cancellation points, and a cancel construct. Cancellation points are implicitly inserted at the following positions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Implicit barriers</p>
</li>
<li>
<p>barrier regions</p>
</li>
<li>
<p>cancel regions</p>
</li>
<li>
<p>cancellation point regions</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>It can be used to solve finder problems where N threads search for a solution and once a solution is found by any thread, all threads end their search.</p>
</div>
<div class="paragraph">
<p>Because there is a performance overhead in checking for cancellations, it is only enabled if the library-internal global variable OMP_CANCELLATION is set. The value of this variable can be checked with the omp_get_cancellation() function, but there is no way modify it from inside the program. It can only be set from the environment when the program is launched.</p>
</div>
<div class="paragraph">
<p>In this example program, once a thread finds the "needle", it signals cancellation for all threads of the current team processing the innermost for loop. Threads check the cancellation only at every loop iteration. It also checks whether OMP_CANCELLATION is set, and if not, sets it and reruns the program.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#include &lt;stdio.h&gt; // For printf
#include &lt;string.h&gt; // For strlen
#include &lt;stdlib.h&gt; // For putenv
#include &lt;unistd.h&gt; // For execv
#include &lt;omp.h&gt; // For omp_get_cancellation, omp_get_thread_num()</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>static const char* FindAnyNeedle(const char* haystack, size_t size, char needle)
{
    const char* result = haystack+size;
    #pragma omp parallel
    {
        unsigned num_iterations=0;
        #pragma omp for
        for(size_t p = 0; p &lt; size; ++p)
        {
            ++num_iterations;
            if(haystack[p] == needle)
            {
                #pragma omp atomic write
                result = haystack+p;
                // Signal cancellation.
                #pragma omp cancel for
            }
            // Check for cancellations signalled by other threads:
            #pragma omp cancellation point for
        }
        // All threads reach here eventually; sooner if the cancellation was signalled.
        printf("Thread %u: %u iterations completed\n", omp_get_thread_num(), num_iterations);
    }
    return result;
}</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>int main(int argc, char** argv)
{
    if(!omp_get_cancellation())
    {
        printf("Cancellations were not enabled, enabling cancellation and rerunning program\n");
        putenv("OMP_CANCELLATION=true");
        execv(argv[0], argv);
    }
    printf("%s\n%*s\n", argv[1], FindAnyNeedle(argv[1],strlen(argv[1]),argv[2][0])-argv[1]+1, "^");
}</pre>
</div>
</div>
<div class="paragraph">
<p>Example output:</p>
</div>
<div class="paragraph">
<div class="title">/a.out "OpenMP cancellations can only be performed synchronously at cancellation points." "l"</div>
<p>Cancellations were not enabled, enabling cancellation and rerunning program</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Thread 0: 10 iterations completed
Thread 1: 3 iterations completed
Thread 7: 10 iterations completed
Thread 3: 10 iterations completed
Thread 4: 10 iterations completed
Thread 2: 8 iterations completed
Thread 5: 5 iterations completed
Thread 6: 6 iterations completed</pre>
</div>
</div>
<div class="paragraph">
<p>OpenMP cancellations can only be performed synchronously at cancellation points.</p>
</div>
<div class="paragraph">
<p>^</p>
</div>
<div class="paragraph">
<p>The keyword in the end of the #pragma omp cancellation point construct is the name of the most closely nested OpenMP construct that you want to cancel. In the example code above, it is the for construct, and this is why the line says #pragma omp cancellation point for.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_loop_nesting"><a class="anchor" href="#_loop_nesting"></a>10. Loop nesting</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_the_problem"><a class="anchor" href="#_the_problem"></a>10.1. The problem</h3>
<div class="paragraph">
<p>A beginner at OpenMP will quickly find out that this code will not do the expected thing:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp parallel for
for(int y=0; y&lt;25; ++y)
{
    #pragma omp parallel for
    for(int x=0; x&lt;80; ++x)
    {
        tick(x,y);
    }
}</pre>
</div>
</div>
<div class="paragraph">
<p>The beginner expects there to be N tick() calls active at the same time (where N = number of processors). Although that is true, the inner loop is not actually parallelised. Only the outer loop is. The inner loop runs in a pure sequence, as if the whole inner #pragma was omitted.</p>
</div>
<div class="paragraph">
<p>At the entrance of the inner parallel construct, the OpenMP runtime library (libgomp in case of GCC) detects that there already exists a team, and instead of a new team of N threads, it will create a team consisting of only the calling thread.</p>
</div>
<div class="paragraph">
<p>Rewriting the code like this won&#8217;t work:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp parallel for
for(int y=0; y&lt;25; ++y)
    {
    #pragma omp for // ERROR, nesting like this is not allowed.
    for(int x=0; x&lt;80; ++x)
    {
        tick(x,y);
    }
}</pre>
</div>
</div>
<div class="paragraph">
<p>This code is erroneous and will cause the program to malfunction. See the restrictions chapter below for details.</p>
</div>
<div class="sect3">
<h4 id="_solution_in_openmp_3_0"><a class="anchor" href="#_solution_in_openmp_3_0"></a>10.1.1. Solution in OpenMP 3.0</h4>
<div class="paragraph">
<p>In OpenMP 3.0, the loop nesting problem can be solved by using the collapse clause in the for construct.</p>
</div>
<div class="paragraph">
<p>Example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#pragma omp parallel for collapse(2)
for(int y=0; y&lt;25; ++y)
    for(int x=0; x&lt;80; ++x)
    {
        tick(x,y);
    }</pre>
</div>
</div>
<div class="paragraph">
<p>The number specified in the collapse clauses is the number of nested loops that are subject to the work-sharing semantics of the OpenMP for construct.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_restrictions"><a class="anchor" href="#_restrictions"></a>10.2. Restrictions</h3>
<div class="paragraph">
<p>There are restrictions to which clauses can be nested under which constructs. The restrictions are listed in the OpenMP official specification.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_performance"><a class="anchor" href="#_performance"></a>11. Performance</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Compared to a naive use of C++11 threads, OpenMP threads are often more efficient. This is because many implementations of OpenMP use a <em>thread pool</em>. A thread pool means that new operating system threads are only created once. When the threads are done with their work, they return to a “dock” waiting for new work to do.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_shortcomings"><a class="anchor" href="#_shortcomings"></a>12. Shortcomings</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_openmp_and_fork"><a class="anchor" href="#_openmp_and_fork"></a>12.1. OpenMP and fork()</h3>
<div class="paragraph">
<p>It is worth mentioning that using OpenMP in a program that calls fork() requires special consideration.</p>
</div>
<div class="paragraph">
<p>This problem only affects GCC; ICC is not affected.</p>
</div>
<div class="paragraph">
<p>If your program intends to become a background process using daemonize() or other similar means, you must not use the OpenMP features <em>before</em> the fork. After OpenMP features are utilized, a fork is only allowed if the child process does not use OpenMP features, or it does so as a completely new process (such as after exec()).</p>
</div>
<div class="paragraph">
<p>This is an example of an erroneous program:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>#include &lt;stdio.h&gt;
#include &lt;sys/wait.h&gt;
#include &lt;unistd.h&gt;
void a()
{
    #pragma omp parallel num_threads(2)
    {
        puts("para_a"); // output twice
    }
    puts("a ended"); // output once
}
void b()
{
    #pragma omp parallel num_threads(2)
    {
    puts("para_b");
    }
    puts("b ended");
}</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>int main() {
    a(); // Invokes OpenMP features (parent process)
    int p = fork();
    if(!p)
    {
        b(); // ERROR: Uses OpenMP again, but in child process
        _exit(0);
    }
    wait(NULL);
    return 0;
}</pre>
</div>
</div>
<div class="paragraph">
<p>When run, this program hangs, never reaching the line that outputs "b ended".</p>
</div>
<div class="paragraph">
<p>There is currently no workaround; the libgomp API does not specify functions that can be used to prepare for a call to fork().</p>
</div>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer" style="border-top: 2px solid #e9e9e9; background-color: #fafafa; padding-bottom: 2em; padding-top: 2em;">
    <div class="container" style="display: flex; flex-direction: column; align-items: center; gap: 0.5em;">
        <div>
            <a href="https://www.cemosis.fr">
                <img src="../_/img/cemosis-logo.svg" alt="Cemosis logo" height="50">
            </a>
        </div>
        <span style="font-size: 0.8rem; color: #9e9e9e">© 2023 <a href="https://www.cemosis.fr" style="text-decoration: underline;">Cemosis</a>, Université de Strasbourg</span>
    </div>
</footer>
<script id="site-script" src="../_/js/site.js" data-ui-root-path="../_"></script>


<script async src="../_/js/vendor/fontawesome-icon-defs.js"></script>
<script async src="../_/js/vendor/fontawesome.js"></script>
<script async src="../_/js/vendor/highlight.js"></script>


<script type="text/javascript">
function toggleFullScreen() {
   var doc = window.document;
   var docEl = doc.documentElement;

   var requestFullScreen = docEl.requestFullscreen || docEl.mozRequestFullScreen || docEl.webkitRequestFullScreen || docEl.msRequestFullscreen;
   var cancelFullScreen = doc.exitFullscreen || doc.mozCancelFullScreen || doc.webkitExitFullscreen || doc.msExitFullscreen;

   if(!doc.fullscreenElement && !doc.mozFullScreenElement && !doc.webkitFullscreenElement && !doc.msFullscreenElement) {
       requestFullScreen.call(docEl);
   }
   else {
       cancelFullScreen.call(doc);
   }
}
</script>
  </body>
</html>
